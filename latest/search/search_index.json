{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"connector/overview/","text":"HStream Connector Overview \u00b6 Stream Database should work with other external systems like messaging systems and other database easily. HStream Connector enable you to easily feed data from external systems into HStreamDB or feed data from HStreamDB into external systems. Concept \u00b6 HStream connectors should include two types of connectors: source and sink. Source Connector - A source connector subscribes data from other systems like mysql or clickhouse, making the data availlable for hstream data processing. Sink Connector - A sink connector write data to other systems like mysql or clickhouse. It often subscribes data from stream in HStreamDB.","title":"Overview"},{"location":"connector/overview/#hstream-connector-overview","text":"Stream Database should work with other external systems like messaging systems and other database easily. HStream Connector enable you to easily feed data from external systems into HStreamDB or feed data from HStreamDB into external systems.","title":"HStream Connector Overview"},{"location":"connector/overview/#concept","text":"HStream connectors should include two types of connectors: source and sink. Source Connector - A source connector subscribes data from other systems like mysql or clickhouse, making the data availlable for hstream data processing. Sink Connector - A sink connector write data to other systems like mysql or clickhouse. It often subscribes data from stream in HStreamDB.","title":"Concept"},{"location":"connector/use/","text":"How to use HStream connectors \u00b6 This tutorial describes how to use HStream connectors. Note Up to the present, we only provide two built-in sink connectors which can subscribe data from streams and write them into mysql and clickhouse. Prerequisites \u00b6 Make sure you have HStreamDB running and accessible. Also available mysql and clickhouse service are required if you want to use related connectors. Create Built-In Connectors \u00b6 Currently, we provide two built-in connectors, mysql sink connector and clickhouse sink connector. Synopsis \u00b6 CREATE SINK CONNECTOR connector_name [ IF NOT EXIST ] WITH ( connector_options [...]); There are some connector options provided to create connector such as database password, database name, username, database host and port. Type is the only required connector option. Until now, the only supported type values are mysql and clickhouse . Mysql Connector \u00b6 connector options and default value for mysql includes: Option Type Default Value username String \"root\" password String \"password\" host String \"127.0.0.1\" port Int None (Required) database String \"mysql\" An example to create a mysql connector which subscribes from stream source and write data to mysql looks like: CREATE SINK CONNECTOR mysql WITH ( type = mysql , host = \"127.0.0.1\" , port = 3306 , stream = source ); The mysql table name will be the same as the stream name. Clickhouse Connector \u00b6 connector options and default value for clickhouse includes: username = \"default\" password = \"\" host = \"127.0.0.1\" port = 9000 database = \"default\" An example to create a clickhouse connector which subscribes from stream source and write data to clickhouse looks like: CREATE SINK CONNECTOR mysql WITH ( type = clickhouse , host = \"127.0.0.1\" , port = 9000 , stream = source ); The clickhouse table name will be the same as the stream name. Check the result \u00b6 Once you create a connector which subscribe a particular stream and write data to external systems, each time there are new data insert into the stream, the data will be write into connected external system in a very short time gap. For example, if you create mysql connector like this: CREATE SINK CONNECTOR mysql WITH ( type = mysql , host = \"127.0.0.1\" , port = 3306 , stream = source ); Since the stream data shema is always known advanced, we assume the mysql table named source is created advanced. The table schema should looks as the same of stream. In this example, we will insert data looks like (temperature, humidity) VALUES (12, 84) . Both temperature and humidity are numbers. So if the mysql table is not exist in the default database mysql yet, you might create the mysql table in this way: use mysql ; CREATE TABLE IF NOT EXISTS source ( temperature INT , humidity INT ) CHARACTER SET utf8 ; After you insert some data in streams by: INSERT INTO source ( temperature , humidity ) VALUES ( 12 , 84 ); INSERT INTO source ( temperature , humidity ) VALUES ( 13 , 83 ); INSERT INTO source ( temperature , humidity ) VALUES ( 14 , 82 ); You should found your data in mysql table very soon after the insertion are done.","title":"Use"},{"location":"connector/use/#how-to-use-hstream-connectors","text":"This tutorial describes how to use HStream connectors. Note Up to the present, we only provide two built-in sink connectors which can subscribe data from streams and write them into mysql and clickhouse.","title":"How to use HStream connectors"},{"location":"connector/use/#prerequisites","text":"Make sure you have HStreamDB running and accessible. Also available mysql and clickhouse service are required if you want to use related connectors.","title":"Prerequisites"},{"location":"connector/use/#create-built-in-connectors","text":"Currently, we provide two built-in connectors, mysql sink connector and clickhouse sink connector.","title":"Create Built-In Connectors"},{"location":"connector/use/#synopsis","text":"CREATE SINK CONNECTOR connector_name [ IF NOT EXIST ] WITH ( connector_options [...]); There are some connector options provided to create connector such as database password, database name, username, database host and port. Type is the only required connector option. Until now, the only supported type values are mysql and clickhouse .","title":"Synopsis"},{"location":"connector/use/#mysql-connector","text":"connector options and default value for mysql includes: Option Type Default Value username String \"root\" password String \"password\" host String \"127.0.0.1\" port Int None (Required) database String \"mysql\" An example to create a mysql connector which subscribes from stream source and write data to mysql looks like: CREATE SINK CONNECTOR mysql WITH ( type = mysql , host = \"127.0.0.1\" , port = 3306 , stream = source ); The mysql table name will be the same as the stream name.","title":"Mysql Connector"},{"location":"connector/use/#clickhouse-connector","text":"connector options and default value for clickhouse includes: username = \"default\" password = \"\" host = \"127.0.0.1\" port = 9000 database = \"default\" An example to create a clickhouse connector which subscribes from stream source and write data to clickhouse looks like: CREATE SINK CONNECTOR mysql WITH ( type = clickhouse , host = \"127.0.0.1\" , port = 9000 , stream = source ); The clickhouse table name will be the same as the stream name.","title":"Clickhouse Connector"},{"location":"connector/use/#check-the-result","text":"Once you create a connector which subscribe a particular stream and write data to external systems, each time there are new data insert into the stream, the data will be write into connected external system in a very short time gap. For example, if you create mysql connector like this: CREATE SINK CONNECTOR mysql WITH ( type = mysql , host = \"127.0.0.1\" , port = 3306 , stream = source ); Since the stream data shema is always known advanced, we assume the mysql table named source is created advanced. The table schema should looks as the same of stream. In this example, we will insert data looks like (temperature, humidity) VALUES (12, 84) . Both temperature and humidity are numbers. So if the mysql table is not exist in the default database mysql yet, you might create the mysql table in this way: use mysql ; CREATE TABLE IF NOT EXISTS source ( temperature INT , humidity INT ) CHARACTER SET utf8 ; After you insert some data in streams by: INSERT INTO source ( temperature , humidity ) VALUES ( 12 , 84 ); INSERT INTO source ( temperature , humidity ) VALUES ( 13 , 83 ); INSERT INTO source ( temperature , humidity ) VALUES ( 14 , 82 ); You should found your data in mysql table very soon after the insertion are done.","title":"Check the result"},{"location":"contributing/build-from-source/","text":"Building from source \u00b6 This document describes how to build HStreamDB from source code. Building with cabal \u00b6 TODO Building with stack \u00b6 TODO","title":"Building from source"},{"location":"contributing/build-from-source/#building-from-source","text":"This document describes how to build HStreamDB from source code.","title":"Building from source"},{"location":"contributing/build-from-source/#building-with-cabal","text":"TODO","title":"Building with cabal"},{"location":"contributing/build-from-source/#building-with-stack","text":"TODO","title":"Building with stack"},{"location":"contributing/haskell-style/","text":"Haskell Style Guide \u00b6 This document is a slightly modified version of style guide used in Kowainik . Style guide goals \u00b6 The purpose of this document is to help developers and people working on Haskell code-bases to have a smoother experience while dealing with code in different situations. This style guide aims to increase productivity by defining the following goals: Make code easier to understand: ideas for solutions should not be hidden behind complex and obscure code. Make code easier to read: code arrangement should be immediately apparent after looking at the existing code. Names of functions & variables should be transparent and obvious. Make code easier to write: developers should think about code formatting rules as little as possible. The style guide should answer any query pertaining to the formatting of a specific piece of code. Make code easier to maintain: this style guide aims to reduce the burden of maintaining packages using version control systems unless this conflicts with the previous points. Rule of thumb when working with existing source code The general rule is to stick to the same coding style that is already used in the file you are editing. If you must make significant style modifications, then commit them independently from the functional changes so that someone looking back through the changelog can easily distinguish between them. Indentation \u00b6 Indent code blocks with 2 spaces . Always put a where keyword on a new line. showSign :: Int -> String showSign n | n == 0 = \"Zero\" | n < 0 = \"Negative\" | otherwise = \"Positive\" greet :: IO () greet = do putStrLn \"What is your name?\" name <- getLine putStrLn $ greeting name where greeting :: String -> String greeting name = \"Hey \" ++ name ++ \"!\" Line length \u00b6 The maximum preferred line length is 80 characters . Tip There is no hard rules when it comes to line length. Some lines just have to be a bit longer than usual. However, if your line of code exceeds this limit, try to split code into smaller chunks or break long lines over multiple shorter ones as much as you can. Whitespaces \u00b6 No trailing whitespaces (use some tools to automatically cleanup trailing whitespaces). Surround binary operators with a single space on either side. Alignment \u00b6 Use comma-leading style for formatting module exports, lists, tuples, records, etc. answers :: [ Maybe Int ] answers = [ Just 42 , Just 7 , Nothing ] If a function definition doesn't fit the line limit then align multiple lines according to the same separator like :: , => , -> . -- + Good printQuestion :: Show a => Text -- ^ Question text -> [ a ] -- ^ List of available answers -> IO () -- + Acceptable if function name is short fun :: Show a => Text -- ^ Question text -> [ a ] -- ^ List of available answers -> IO () Align records with every field on a separate line with leading commas. -- + Good data Foo = Foo { fooBar :: Bar , fooBaz :: Baz , fooQuux :: Quux } deriving ( Eq , Show , Generic ) deriving anyclass ( FromJSON , ToJSON ) -- + Acceptable data Foo = Foo { fooBar :: Bar , fooBaz :: Baz , fooQuux :: Quux } deriving ( Eq , Show , Generic ) deriving anyclass ( FromJSON , ToJSON ) Align sum types with every constructor on its own line with leading = and | . -- + Good data TrafficLight = Red | Yellow | Green deriving ( Eq , Ord , Enum , Bounded , Show , Read ) -- + Acceptable data TrafficLight = Red | Yellow | Green deriving ( Eq , Ord , Enum , Bounded , Show , Read ) Try to follow the above rule inside function definitions but without fanatism: -- + Good createFoo = Foo <$> veryLongBar <*> veryLongBaz -- + Acceptable createFoo = Foo <$> veryLongBar <*> veryLongBaz -- + Acceptable createFoo = Foo <$> veryLongBar <*> veryLongBaz -- - Bad createFoo = Foo <$> veryLongBar <*> veryLongBaz -- - Bad createFoo = Foo -- there's no need to put the constructor on a separate line and have an extra line <$> veryLongBar <*> veryLongBaz Basically, it is often possible to join consequent lines without introducing alignment dependency. Try not to span multiple short lines unnecessarily. If a function application must spawn multiple lines to fit within the maximum line length, then write one argument on each line following the head, indented by one level: veryLongProductionName firstArgumentOfThisFunction secondArgumentOfThisFunction ( DummyDatatype withDummyField1 andDummyField2 ) lastArgumentOfThisFunction Naming \u00b6 Functions and variables \u00b6 lowerCamelCase for function and variable names. UpperCamelCase for data types, typeclasses and constructors. Variant Use ids_with_underscores for local variables only. Try not to create new operators. -- What does this 'mouse operator' mean? :thinking_suicide: ( ~@@^> ) :: Functor f => ( a -> b ) -> ( a -> c -> d ) -> ( b -> f c ) -> a -> f d Do not use ultra-short or indescriptive names like a , par , g unless the types of these variables are general enough. -- + Good mapSelect :: forall a . ( a -> Bool ) -> ( a -> a ) -> ( a -> a ) -> [ a ] -> [ a ] mapSelect test ifTrue ifFalse = go where go :: [ a ] -> [ a ] go [] = [] go ( x : xs ) = if test x then ifTrue x : go xs else ifFalse x : go xs -- - Bad mapSelect :: forall a . ( a -> Bool ) -> ( a -> a ) -> ( a -> a ) -> [ a ] -> [ a ] mapSelect p f g = go where go :: [ a ] -> [ a ] go [] = [] go ( x : xs ) = if p x then f x : go xs else g x : go xs Do not introduce unnecessarily long names for variables. -- + Good map :: ( a -> b ) -> [ a ] -> [ b ] map _ [] = [] map f ( x : xs ) = f x : map f xs -- - Bad map :: ( a -> b ) -> [ a ] -> [ b ] map _ [] = [] map function ( firstElement : remainingList ) = function firstElement : map function remainingList For readability reasons, do not capitalize all letters when using an abbreviation as a part of a longer name. For example, write TomlException instead of TOMLException . Unicode symbols are allowed only in modules that already use unicode symbols. If you create a unicode name, you should also create a non-unicode one as an alias. Data types \u00b6 Creating data types is extremely easy in Haskell. It is usually a good idea to introduce a custom data type (enum or newtype ) instead of using a commonly used data type (like Int , String , Set Text , etc.). type aliases are allowed only for specializing general types: -- + Good data StateT s m a type State s = StateT s Identity -- - Bad type Size = Int Use the data type name as the constructor name for data with single constructor and newtype . data User = User Int String The field name for a newtype must be prefixed by un followed by the type name. newtype Size = Size { unSize :: Int } newtype App a = App { unApp :: ReaderT Context IO a } Field names for the record data type should start with the full name of the data type. -- + Good data HealthReading = HealthReading { healthReadingDate :: UTCTime , healthReadingMeasurement :: Double } It is acceptable to use an abbreviation as the field prefix if the data type name is too long. -- + Acceptable data HealthReading = HealthReading { hrDate :: UTCTime , hrMeasurement :: Double } Comments \u00b6 Separate end-of-line comments from the code with 2 spaces . newtype Measure = Measure { unMeasure :: Double -- ^ See how 2 spaces separate this comment } Write Haddock documentation for the top-level functions, function arguments and data type fields. The documentation should give enough information to apply the function without looking at its definition. -- | Single-line short comment. foo :: Int -> [ a ] -> [ a ] -- | Example of multi-line block comment which is very long -- and doesn't fit single line. foo :: Int -> [ a ] -> [ a ] -- + Good -- | 'replicate' @n x@ returns list of length @n@ with @x@ as the value of -- every element. This function is lazy in its returned value. replicate :: Int -- ^ Length of returned list -> a -- ^ Element to populate list -> [ a ] -- - Bad -- | 'replicate' @n x@ returns list of length @n@ with @x@ as the value of -- every element. This function is lazy in its returned value. replicate :: Int -- ^ Length of returned list {- | Element to populate list -} -> a -> [ a ] If possible, include typeclass laws and function usage examples into the documentation. -- | The class of semigroups (types with an associative binary operation). -- -- Instances should satisfy the associativity law: -- -- * @x '<>' (y '<>' z) = (x '<>' y) '<>' z@ class Semigroup a where ( <> ) :: a -> a -> a -- | The 'intersperse' function takes a character and places it -- between the characters of a 'Text'. -- -- >>> T.intersperse '.' \"SHIELD\" -- \"S.H.I.E.L.D\" intersperse :: Char -> Text -> Text Guideline for module formatting \u00b6 Allowed tools for automatic module formatting: stylish-haskell : for formatting the import section and for alignment. LANGUAGE \u00b6 Put OPTIONS_GHC pragma before LANGUAGE pragmas in a separate section. Write each LANGUAGE pragma on its own line, sort them alphabetically and align by max width among them. {-# OPTIONS_GHC -fno-warn-orphans #-} {-# LANGUAGE ApplicativeDo #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE TypeApplications #-} Always put language extensions in the relevant source file. Tip Language extensions must be listed at the very top of the file, above the module name. Export lists \u00b6 Use the following rules to format the export section: Always write an explicit export list. Indent the export list by 2 spaces . You can split the export list into sections. Use Haddock to assign names to these sections. Classes, data types and type aliases should be written before functions in each section. module Map ( -- * Data type Map , Key , empty -- * Update , insert , insertWith , alter ) where Imports \u00b6 Always use explicit import lists or qualified imports. Use qualified imports only if the import list is big enough or there are conflicts in names. This makes the code more robust against changes in dependent libraries. Exception: modules that only reexport other entire modules. Imports should be grouped in the following order: Imports from Hackage packages. Imports from the current project. Put a blank line between each group of imports. The imports in each group should be sorted alphabetically by module name. module MyProject.Foo ( Foo ( .. ) ) where import Control.Exception ( catch , try ) import qualified Data.Aeson as Json import qualified Data.Text as Text import Data.Traversable ( for ) import MyProject.Ansi ( errorMessage , infoMessage ) import qualified MyProject.BigModule as Big data Foo ... Data declaration \u00b6 Refer to the Alignment section to see how to format data type declarations. Records for data types with multiple constructors are forbidden. -- - Bad data Foo = Bar { bar1 :: Int , bar2 :: Double } | Baz { baz1 :: Int , baz2 :: Double , baz3 :: Text } -- + Good data Foo = FooBar Bar | FooBaz Baz data Bar = Bar { bar1 :: Int , bar2 :: Double } data Baz = Baz { baz1 :: Int , baz2 :: Double , baz3 :: Text } -- + Also good data Foo = Bar Int Double | Baz Int Double Text Strictness \u00b6 Fields of data type constructors should be strict. Specify strictness explicitly with ! . This helps to avoid space leaks and gives you an error instead of a warning in case you forget to initialize some fields. -- + Good data Settings = Settings { settingsHasTravis :: ! Bool , settingsConfigPath :: ! FilePath , settingsRetryCount :: ! Int } -- - Bad data Settings = Settings { settingsHasTravis :: Bool , settingsConfigPath :: FilePath , settingsRetryCount :: Int } Deriving \u00b6 Type classes in the deriving section should always be surrounded by parentheses. Don't derive typeclasses unnecessarily. Use -XDerivingStrategies extension for newtype s to explicitly specify the way you want to derive type classes: {-# LANGUAGE DeriveAnyClass #-} {-# LANGUAGE DerivingStrategies #-} {-# LANGUAGE GeneralizedNewtypeDeriving #-} newtype Id a = Id { unId :: Int } deriving stock ( Generic ) deriving newtype ( Eq , Ord , Show , Hashable ) deriving anyclass ( FromJSON , ToJSON ) Function declaration \u00b6 All top-level functions must have type signatures. All functions inside a where block must have type signatures. Explicit type signatures help to avoid cryptic type errors. You might need the -XScopedTypeVariables extension to write the polymorphic types of functions inside a where block. Surround . after forall in type signatures with spaces. lookup :: forall a f . Typeable a => TypeRepMap f -> Maybe ( f a ) If the function type signature is very long, then place the type of each argument under its own line with respect to alignment. sendEmail :: forall env m . ( MonadLog m , MonadEmail m , WithDb env m ) => Email -> Subject -> Body -> Template -> m () If the line with argument names is too big, then put each argument on its own line and separate it somehow from the body section. sendEmail toEmail subject @ ( Subject subj ) body Template { .. } -- default body variables = do < code goes here > In other cases, place an = sign on the same line where the function definition is. Put operator fixity before operator signature: -- | Flipped version of '<$>'. infixl 1 <&> ( <&> ) :: Functor f => f a -> ( a -> b ) -> f b as <&> f = f <$> as Put pragmas immediately following the function they apply to. -- | Lifted version of 'T.putStrLn'. putTextLn :: MonadIO m => Text -> m () putTextLn = liftIO . Text . putStrLn {-# INLINE putTextLn #-} {-# SPECIALIZE putTextLn :: Text -> IO () #-} In case of data type definitions, you must put the pragma before the type it applies to. Example: data TypeRepMap ( f :: k -> Type ) = TypeRepMap { fingerprintAs :: {-# UNPACK #-} ! ( PrimArray Word64 ) , fingerprintBs :: {-# UNPACK #-} ! ( PrimArray Word64 ) , trAnys :: {-# UNPACK #-} ! ( Array Any ) , trKeys :: {-# UNPACK #-} ! ( Array Any ) } if-then-else clauses \u00b6 Prefer guards over if-then-else where possible. -- + Good showParity :: Int -> Bool showParity n | even n = \"even\" | otherwise = \"odd\" -- - Meh showParity :: Int -> Bool showParity n = if even n then \"even\" else \"odd\" In the code outside do -blocks you can align if-then-else clauses like you would normal expressions: shiftInts :: [ Int ] -> [ Int ] shiftInts = map $ \\ n -> if even n then n + 1 else n - 1 Case expressions \u00b6 Align the -> arrows in the alternatives when it helps readability. -- + Good firstOrDefault :: [ a ] -> a -> a firstOrDefault list def = case list of [] -> def x : _ -> x -- - Bad foo :: IO () foo = getArgs >>= \\ case [] -> do putStrLn \"No arguments provided\" runWithNoArgs firstArg : secondArg : rest -> do putStrLn $ \"The first argument is \" ++ firstArg putStrLn $ \"The second argument is \" ++ secondArg _ -> pure () Use the -XLambdaCase extension when you perform pattern matching over the last argument of the function: fromMaybe :: a -> Maybe a -> a fromMaybe v = \\ case Nothing -> v Just x -> x let expressions \u00b6 Write every let -binding on a new line: isLimitedBy :: Integer -> Natural -> Bool isLimitedBy n limit = let intLimit = toInteger limit in n <= intLimit Put a let before each variable inside a do block. General recommendations \u00b6 Try to split code into separate modules. Avoid abusing point-free style. Sometimes code is clearer when not written in point-free style: -- + Good foo :: Int -> a -> Int foo n x = length $ replicate n x -- - Bad foo :: Int -> a -> Int foo = ( length . ) . replicate Code should be compilable with the following ghc options without warnings: -Wall -Wincomplete-uni-patterns -Wincomplete-record-updates -Wcompat -Widentities -Wredundant-constraints -Wmissing-export-lists -Wpartial-fields Enable -fhide-source-paths and -freverse-errors for cleaner compiler output. Use -XApplicativeDo in combination with -XRecordWildCards to prevent position-sensitive errors where possible.","title":"Haskell style"},{"location":"contributing/haskell-style/#haskell-style-guide","text":"This document is a slightly modified version of style guide used in Kowainik .","title":"Haskell Style Guide"},{"location":"contributing/haskell-style/#style-guide-goals","text":"The purpose of this document is to help developers and people working on Haskell code-bases to have a smoother experience while dealing with code in different situations. This style guide aims to increase productivity by defining the following goals: Make code easier to understand: ideas for solutions should not be hidden behind complex and obscure code. Make code easier to read: code arrangement should be immediately apparent after looking at the existing code. Names of functions & variables should be transparent and obvious. Make code easier to write: developers should think about code formatting rules as little as possible. The style guide should answer any query pertaining to the formatting of a specific piece of code. Make code easier to maintain: this style guide aims to reduce the burden of maintaining packages using version control systems unless this conflicts with the previous points. Rule of thumb when working with existing source code The general rule is to stick to the same coding style that is already used in the file you are editing. If you must make significant style modifications, then commit them independently from the functional changes so that someone looking back through the changelog can easily distinguish between them.","title":"Style guide goals"},{"location":"contributing/haskell-style/#indentation","text":"Indent code blocks with 2 spaces . Always put a where keyword on a new line. showSign :: Int -> String showSign n | n == 0 = \"Zero\" | n < 0 = \"Negative\" | otherwise = \"Positive\" greet :: IO () greet = do putStrLn \"What is your name?\" name <- getLine putStrLn $ greeting name where greeting :: String -> String greeting name = \"Hey \" ++ name ++ \"!\"","title":"Indentation"},{"location":"contributing/haskell-style/#line-length","text":"The maximum preferred line length is 80 characters . Tip There is no hard rules when it comes to line length. Some lines just have to be a bit longer than usual. However, if your line of code exceeds this limit, try to split code into smaller chunks or break long lines over multiple shorter ones as much as you can.","title":"Line length"},{"location":"contributing/haskell-style/#whitespaces","text":"No trailing whitespaces (use some tools to automatically cleanup trailing whitespaces). Surround binary operators with a single space on either side.","title":"Whitespaces"},{"location":"contributing/haskell-style/#alignment","text":"Use comma-leading style for formatting module exports, lists, tuples, records, etc. answers :: [ Maybe Int ] answers = [ Just 42 , Just 7 , Nothing ] If a function definition doesn't fit the line limit then align multiple lines according to the same separator like :: , => , -> . -- + Good printQuestion :: Show a => Text -- ^ Question text -> [ a ] -- ^ List of available answers -> IO () -- + Acceptable if function name is short fun :: Show a => Text -- ^ Question text -> [ a ] -- ^ List of available answers -> IO () Align records with every field on a separate line with leading commas. -- + Good data Foo = Foo { fooBar :: Bar , fooBaz :: Baz , fooQuux :: Quux } deriving ( Eq , Show , Generic ) deriving anyclass ( FromJSON , ToJSON ) -- + Acceptable data Foo = Foo { fooBar :: Bar , fooBaz :: Baz , fooQuux :: Quux } deriving ( Eq , Show , Generic ) deriving anyclass ( FromJSON , ToJSON ) Align sum types with every constructor on its own line with leading = and | . -- + Good data TrafficLight = Red | Yellow | Green deriving ( Eq , Ord , Enum , Bounded , Show , Read ) -- + Acceptable data TrafficLight = Red | Yellow | Green deriving ( Eq , Ord , Enum , Bounded , Show , Read ) Try to follow the above rule inside function definitions but without fanatism: -- + Good createFoo = Foo <$> veryLongBar <*> veryLongBaz -- + Acceptable createFoo = Foo <$> veryLongBar <*> veryLongBaz -- + Acceptable createFoo = Foo <$> veryLongBar <*> veryLongBaz -- - Bad createFoo = Foo <$> veryLongBar <*> veryLongBaz -- - Bad createFoo = Foo -- there's no need to put the constructor on a separate line and have an extra line <$> veryLongBar <*> veryLongBaz Basically, it is often possible to join consequent lines without introducing alignment dependency. Try not to span multiple short lines unnecessarily. If a function application must spawn multiple lines to fit within the maximum line length, then write one argument on each line following the head, indented by one level: veryLongProductionName firstArgumentOfThisFunction secondArgumentOfThisFunction ( DummyDatatype withDummyField1 andDummyField2 ) lastArgumentOfThisFunction","title":"Alignment"},{"location":"contributing/haskell-style/#naming","text":"","title":"Naming"},{"location":"contributing/haskell-style/#functions-and-variables","text":"lowerCamelCase for function and variable names. UpperCamelCase for data types, typeclasses and constructors. Variant Use ids_with_underscores for local variables only. Try not to create new operators. -- What does this 'mouse operator' mean? :thinking_suicide: ( ~@@^> ) :: Functor f => ( a -> b ) -> ( a -> c -> d ) -> ( b -> f c ) -> a -> f d Do not use ultra-short or indescriptive names like a , par , g unless the types of these variables are general enough. -- + Good mapSelect :: forall a . ( a -> Bool ) -> ( a -> a ) -> ( a -> a ) -> [ a ] -> [ a ] mapSelect test ifTrue ifFalse = go where go :: [ a ] -> [ a ] go [] = [] go ( x : xs ) = if test x then ifTrue x : go xs else ifFalse x : go xs -- - Bad mapSelect :: forall a . ( a -> Bool ) -> ( a -> a ) -> ( a -> a ) -> [ a ] -> [ a ] mapSelect p f g = go where go :: [ a ] -> [ a ] go [] = [] go ( x : xs ) = if p x then f x : go xs else g x : go xs Do not introduce unnecessarily long names for variables. -- + Good map :: ( a -> b ) -> [ a ] -> [ b ] map _ [] = [] map f ( x : xs ) = f x : map f xs -- - Bad map :: ( a -> b ) -> [ a ] -> [ b ] map _ [] = [] map function ( firstElement : remainingList ) = function firstElement : map function remainingList For readability reasons, do not capitalize all letters when using an abbreviation as a part of a longer name. For example, write TomlException instead of TOMLException . Unicode symbols are allowed only in modules that already use unicode symbols. If you create a unicode name, you should also create a non-unicode one as an alias.","title":"Functions and variables"},{"location":"contributing/haskell-style/#data-types","text":"Creating data types is extremely easy in Haskell. It is usually a good idea to introduce a custom data type (enum or newtype ) instead of using a commonly used data type (like Int , String , Set Text , etc.). type aliases are allowed only for specializing general types: -- + Good data StateT s m a type State s = StateT s Identity -- - Bad type Size = Int Use the data type name as the constructor name for data with single constructor and newtype . data User = User Int String The field name for a newtype must be prefixed by un followed by the type name. newtype Size = Size { unSize :: Int } newtype App a = App { unApp :: ReaderT Context IO a } Field names for the record data type should start with the full name of the data type. -- + Good data HealthReading = HealthReading { healthReadingDate :: UTCTime , healthReadingMeasurement :: Double } It is acceptable to use an abbreviation as the field prefix if the data type name is too long. -- + Acceptable data HealthReading = HealthReading { hrDate :: UTCTime , hrMeasurement :: Double }","title":"Data types"},{"location":"contributing/haskell-style/#comments","text":"Separate end-of-line comments from the code with 2 spaces . newtype Measure = Measure { unMeasure :: Double -- ^ See how 2 spaces separate this comment } Write Haddock documentation for the top-level functions, function arguments and data type fields. The documentation should give enough information to apply the function without looking at its definition. -- | Single-line short comment. foo :: Int -> [ a ] -> [ a ] -- | Example of multi-line block comment which is very long -- and doesn't fit single line. foo :: Int -> [ a ] -> [ a ] -- + Good -- | 'replicate' @n x@ returns list of length @n@ with @x@ as the value of -- every element. This function is lazy in its returned value. replicate :: Int -- ^ Length of returned list -> a -- ^ Element to populate list -> [ a ] -- - Bad -- | 'replicate' @n x@ returns list of length @n@ with @x@ as the value of -- every element. This function is lazy in its returned value. replicate :: Int -- ^ Length of returned list {- | Element to populate list -} -> a -> [ a ] If possible, include typeclass laws and function usage examples into the documentation. -- | The class of semigroups (types with an associative binary operation). -- -- Instances should satisfy the associativity law: -- -- * @x '<>' (y '<>' z) = (x '<>' y) '<>' z@ class Semigroup a where ( <> ) :: a -> a -> a -- | The 'intersperse' function takes a character and places it -- between the characters of a 'Text'. -- -- >>> T.intersperse '.' \"SHIELD\" -- \"S.H.I.E.L.D\" intersperse :: Char -> Text -> Text","title":"Comments"},{"location":"contributing/haskell-style/#guideline-for-module-formatting","text":"Allowed tools for automatic module formatting: stylish-haskell : for formatting the import section and for alignment.","title":"Guideline for module formatting"},{"location":"contributing/haskell-style/#language","text":"Put OPTIONS_GHC pragma before LANGUAGE pragmas in a separate section. Write each LANGUAGE pragma on its own line, sort them alphabetically and align by max width among them. {-# OPTIONS_GHC -fno-warn-orphans #-} {-# LANGUAGE ApplicativeDo #-} {-# LANGUAGE ScopedTypeVariables #-} {-# LANGUAGE TypeApplications #-} Always put language extensions in the relevant source file. Tip Language extensions must be listed at the very top of the file, above the module name.","title":"LANGUAGE"},{"location":"contributing/haskell-style/#export-lists","text":"Use the following rules to format the export section: Always write an explicit export list. Indent the export list by 2 spaces . You can split the export list into sections. Use Haddock to assign names to these sections. Classes, data types and type aliases should be written before functions in each section. module Map ( -- * Data type Map , Key , empty -- * Update , insert , insertWith , alter ) where","title":"Export lists"},{"location":"contributing/haskell-style/#imports","text":"Always use explicit import lists or qualified imports. Use qualified imports only if the import list is big enough or there are conflicts in names. This makes the code more robust against changes in dependent libraries. Exception: modules that only reexport other entire modules. Imports should be grouped in the following order: Imports from Hackage packages. Imports from the current project. Put a blank line between each group of imports. The imports in each group should be sorted alphabetically by module name. module MyProject.Foo ( Foo ( .. ) ) where import Control.Exception ( catch , try ) import qualified Data.Aeson as Json import qualified Data.Text as Text import Data.Traversable ( for ) import MyProject.Ansi ( errorMessage , infoMessage ) import qualified MyProject.BigModule as Big data Foo ...","title":"Imports"},{"location":"contributing/haskell-style/#data-declaration","text":"Refer to the Alignment section to see how to format data type declarations. Records for data types with multiple constructors are forbidden. -- - Bad data Foo = Bar { bar1 :: Int , bar2 :: Double } | Baz { baz1 :: Int , baz2 :: Double , baz3 :: Text } -- + Good data Foo = FooBar Bar | FooBaz Baz data Bar = Bar { bar1 :: Int , bar2 :: Double } data Baz = Baz { baz1 :: Int , baz2 :: Double , baz3 :: Text } -- + Also good data Foo = Bar Int Double | Baz Int Double Text","title":"Data declaration"},{"location":"contributing/haskell-style/#strictness","text":"Fields of data type constructors should be strict. Specify strictness explicitly with ! . This helps to avoid space leaks and gives you an error instead of a warning in case you forget to initialize some fields. -- + Good data Settings = Settings { settingsHasTravis :: ! Bool , settingsConfigPath :: ! FilePath , settingsRetryCount :: ! Int } -- - Bad data Settings = Settings { settingsHasTravis :: Bool , settingsConfigPath :: FilePath , settingsRetryCount :: Int }","title":"Strictness"},{"location":"contributing/haskell-style/#deriving","text":"Type classes in the deriving section should always be surrounded by parentheses. Don't derive typeclasses unnecessarily. Use -XDerivingStrategies extension for newtype s to explicitly specify the way you want to derive type classes: {-# LANGUAGE DeriveAnyClass #-} {-# LANGUAGE DerivingStrategies #-} {-# LANGUAGE GeneralizedNewtypeDeriving #-} newtype Id a = Id { unId :: Int } deriving stock ( Generic ) deriving newtype ( Eq , Ord , Show , Hashable ) deriving anyclass ( FromJSON , ToJSON )","title":"Deriving"},{"location":"contributing/haskell-style/#function-declaration","text":"All top-level functions must have type signatures. All functions inside a where block must have type signatures. Explicit type signatures help to avoid cryptic type errors. You might need the -XScopedTypeVariables extension to write the polymorphic types of functions inside a where block. Surround . after forall in type signatures with spaces. lookup :: forall a f . Typeable a => TypeRepMap f -> Maybe ( f a ) If the function type signature is very long, then place the type of each argument under its own line with respect to alignment. sendEmail :: forall env m . ( MonadLog m , MonadEmail m , WithDb env m ) => Email -> Subject -> Body -> Template -> m () If the line with argument names is too big, then put each argument on its own line and separate it somehow from the body section. sendEmail toEmail subject @ ( Subject subj ) body Template { .. } -- default body variables = do < code goes here > In other cases, place an = sign on the same line where the function definition is. Put operator fixity before operator signature: -- | Flipped version of '<$>'. infixl 1 <&> ( <&> ) :: Functor f => f a -> ( a -> b ) -> f b as <&> f = f <$> as Put pragmas immediately following the function they apply to. -- | Lifted version of 'T.putStrLn'. putTextLn :: MonadIO m => Text -> m () putTextLn = liftIO . Text . putStrLn {-# INLINE putTextLn #-} {-# SPECIALIZE putTextLn :: Text -> IO () #-} In case of data type definitions, you must put the pragma before the type it applies to. Example: data TypeRepMap ( f :: k -> Type ) = TypeRepMap { fingerprintAs :: {-# UNPACK #-} ! ( PrimArray Word64 ) , fingerprintBs :: {-# UNPACK #-} ! ( PrimArray Word64 ) , trAnys :: {-# UNPACK #-} ! ( Array Any ) , trKeys :: {-# UNPACK #-} ! ( Array Any ) }","title":"Function declaration"},{"location":"contributing/haskell-style/#if-then-else-clauses","text":"Prefer guards over if-then-else where possible. -- + Good showParity :: Int -> Bool showParity n | even n = \"even\" | otherwise = \"odd\" -- - Meh showParity :: Int -> Bool showParity n = if even n then \"even\" else \"odd\" In the code outside do -blocks you can align if-then-else clauses like you would normal expressions: shiftInts :: [ Int ] -> [ Int ] shiftInts = map $ \\ n -> if even n then n + 1 else n - 1","title":"if-then-else clauses"},{"location":"contributing/haskell-style/#case-expressions","text":"Align the -> arrows in the alternatives when it helps readability. -- + Good firstOrDefault :: [ a ] -> a -> a firstOrDefault list def = case list of [] -> def x : _ -> x -- - Bad foo :: IO () foo = getArgs >>= \\ case [] -> do putStrLn \"No arguments provided\" runWithNoArgs firstArg : secondArg : rest -> do putStrLn $ \"The first argument is \" ++ firstArg putStrLn $ \"The second argument is \" ++ secondArg _ -> pure () Use the -XLambdaCase extension when you perform pattern matching over the last argument of the function: fromMaybe :: a -> Maybe a -> a fromMaybe v = \\ case Nothing -> v Just x -> x","title":"Case expressions"},{"location":"contributing/haskell-style/#let-expressions","text":"Write every let -binding on a new line: isLimitedBy :: Integer -> Natural -> Bool isLimitedBy n limit = let intLimit = toInteger limit in n <= intLimit Put a let before each variable inside a do block.","title":"let expressions"},{"location":"contributing/haskell-style/#general-recommendations","text":"Try to split code into separate modules. Avoid abusing point-free style. Sometimes code is clearer when not written in point-free style: -- + Good foo :: Int -> a -> Int foo n x = length $ replicate n x -- - Bad foo :: Int -> a -> Int foo = ( length . ) . replicate Code should be compilable with the following ghc options without warnings: -Wall -Wincomplete-uni-patterns -Wincomplete-record-updates -Wcompat -Widentities -Wredundant-constraints -Wmissing-export-lists -Wpartial-fields Enable -fhide-source-paths and -freverse-errors for cleaner compiler output. Use -XApplicativeDo in combination with -XRecordWildCards to prevent position-sensitive errors where possible.","title":"General recommendations"},{"location":"deployment/deploy-k8s/","text":"Running on Kubernetes \u00b6 This document describes how to run HStreamDB kubernetes using the specs that we provide. The document assumes basic previous kubernetes knowledge. By the end of this section, you'll have a fully running HStreamDB cluster on kubernetes that's ready to receive reads/writes, process datas, etc. Building your Kubernetes Cluster \u00b6 The first step is to have a running kubernetes cluster. You can use a managed cluster (provided by your cloud provider), a self-hosted cluster or a local kubernetes cluster using a tool like minikube. Make sure that kubectl points to whatever cluster you're planning to use. Also, you need a storageClass named hstream-store , you can create by kubectl or by your cloud provider web page if it has. Install Zookeeper \u00b6 HStreamDB depends on Zookeeper for storing queries information and some internal storage configuration. So we will need to provision a zookeeper ensemble that HStreamDB will be able to access. For this demo, we will use helm (A package manager for kubernetes) to install zookeeper. After installing helm run: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm install zookeeper bitnami/zookeeper \\ --set image.tag = 3 .6.3 \\ --set replicaCount = 3 \\ --set persistence.storageClass = hstream-store \\ --set persistence.size = 20Gi NAME: zookeeper LAST DEPLOYED: Tue Jul 6 10:51:37 2021 NAMESPACE: test STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** ZooKeeper can be accessed via port 2181 on the following DNS name from within your cluster: zookeeper.svc.cluster.local To connect to your ZooKeeper server run the following commands: export POD_NAME=$(kubectl get pods -l \"app.kubernetes.io/name=zookeeper,app.kubernetes.io/instance=zookeeper,app.kubernetes.io/component=zookeeper\" -o jsonpath=\"{.items[0].metadata.name}\") kubectl exec -it $POD_NAME -- zkCli.sh To connect to your ZooKeeper server from outside the cluster execute the following commands: kubectl port-forward svc/zookeeper 2181:2181 & zkCli.sh 127.0.0.1:2181 WARNING: Rolling tag detected (bitnami/zookeeper:3.6.3), please note that it is strongly recommended to avoid using rolling tags in a production environment. +info https://docs.bitnami.com/containers/how-to/understand-rolling-tags-containers/ This will by default install a 3 nodes zookeeper ensemble. Wait until all the three pods are marked as ready: kubectl get pods NAME READY STATUS RESTARTS AGE zookeeper-0 1/1 Running 0 22h zookeeper-1 1/1 Running 0 4d22h zookeeper-2 1/1 Running 0 16m Configuring and Starting HStreamDB \u00b6 Once all the zookeeper pods are ready, we're ready to start installing the HStreamDB cluster. Fetching The K8s Specs \u00b6 git clone git@github.com:hstreamdb/hstream.git cd hstream/k8s Update Configuration \u00b6 If you used a different way to install zookeeper, make sure to update the zookeeper connection string in storage config file config.json and server service file hstream-server.yaml . It should look something like this: $ cat config.json | grep -A 2 zookeeper \"zookeeper\" : { \"zookeeper_uri\" : \"ip://zookeeper-0.zookeeper-headless:2181,zookeeper-1.zookeeper-headless:2181,zookeeper-2.zookeeper-headless:2181\" , \"timeout\" : \"30s\" } $ cat hstream-server.yaml | grep -A 1 zkuri - \"--zkuri\" - \"zookeeper-0.zookeeper-headless:2181,zookeeper-1.zookeeper-headless:2181,zookeeper-2.zookeeper-headless:2181\" Tips The zookeeper connection string in stotage config file and the service file can be different. But for normal scenario, they are the same. By default, this spec installs a 3 nodes HStream server cluster and 4 nodes storage cluster. If you want a bigger cluster, modify the hstream-server.yaml and logdevice-statefulset.yaml file, and increase the number of replicas to the number of nodes you want in the cluster. Also by default, we attach a 40GB persistent storage to the nodes, if you want more you can change that under the volumeClaimTemplates section. Starting the Cluster \u00b6 kubectl apply -k . When you run kubectl get pods , you should see something like this: NAME READY STATUS RESTARTS AGE hstream-server-deployment-765c84c489-94nqd 1/1 Running 0 6d18h hstream-server-deployment-765c84c489-jrm5p 1/1 Running 0 6d18h hstream-server-deployment-765c84c489-jxsjd 1/1 Running 0 6d18h logdevice-0 1/1 Running 0 6d18h logdevice-1 1/1 Running 0 6d18h logdevice-2 1/1 Running 0 6d18h logdevice-3 1/1 Running 0 6d18h logdevice-admin-server-deployment-5c5fb9f8fb-27jlk 1/1 Running 0 6d18h zookeeper-0 1/1 Running 0 6d22h zookeeper-1 1/1 Running 0 10d zookeeper-2 1/1 Running 0 6d Bootstrapping the Storage Cluster \u00b6 Once all the logdevice pods are running and ready, you'll need to bootstrap the cluster to enable all the nodes. To do that, run: kubectl run hadmin -it --rm --restart = Never --image = hstreamdb/hstream -- \\ hadmin --host logdevice-admin-server-service \\ nodes-config \\ bootstrap --metadata-replicate-across 'node:3' This will start a hadmin pod, that connects to the admin server and invokes the nodes-config bootstrap hadmin command and sets the metadata replication property of the cluster to be replicated across three different nodes. On success, you should see something like: Successfully bootstrapped the cluster pod \"hadmin\" deleted Managing the Storage Cluster \u00b6 kubectl run hadmin -it --rm --restart = Never --image = hstreamdb/hstream -- bash Now you can run hadmin to manage the cluster: hadmin --help To check the state of the cluster, you can then run: hadmin --host logdevice-admin-server-service status +----+-------------+-------+---------------+ | ID | NAME | STATE | HEALTH STATUS | +----+-------------+-------+---------------+ | 0 | logdevice-0 | ALIVE | HEALTHY | | 1 | logdevice-1 | ALIVE | HEALTHY | | 2 | logdevice-2 | ALIVE | HEALTHY | | 3 | logdevice-3 | ALIVE | HEALTHY | +----+-------------+-------+---------------+ Took 2.567s","title":"Running on Kubernetes"},{"location":"deployment/deploy-k8s/#running-on-kubernetes","text":"This document describes how to run HStreamDB kubernetes using the specs that we provide. The document assumes basic previous kubernetes knowledge. By the end of this section, you'll have a fully running HStreamDB cluster on kubernetes that's ready to receive reads/writes, process datas, etc.","title":"Running on Kubernetes"},{"location":"deployment/deploy-k8s/#building-your-kubernetes-cluster","text":"The first step is to have a running kubernetes cluster. You can use a managed cluster (provided by your cloud provider), a self-hosted cluster or a local kubernetes cluster using a tool like minikube. Make sure that kubectl points to whatever cluster you're planning to use. Also, you need a storageClass named hstream-store , you can create by kubectl or by your cloud provider web page if it has.","title":"Building your Kubernetes Cluster"},{"location":"deployment/deploy-k8s/#install-zookeeper","text":"HStreamDB depends on Zookeeper for storing queries information and some internal storage configuration. So we will need to provision a zookeeper ensemble that HStreamDB will be able to access. For this demo, we will use helm (A package manager for kubernetes) to install zookeeper. After installing helm run: helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm install zookeeper bitnami/zookeeper \\ --set image.tag = 3 .6.3 \\ --set replicaCount = 3 \\ --set persistence.storageClass = hstream-store \\ --set persistence.size = 20Gi NAME: zookeeper LAST DEPLOYED: Tue Jul 6 10:51:37 2021 NAMESPACE: test STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ** Please be patient while the chart is being deployed ** ZooKeeper can be accessed via port 2181 on the following DNS name from within your cluster: zookeeper.svc.cluster.local To connect to your ZooKeeper server run the following commands: export POD_NAME=$(kubectl get pods -l \"app.kubernetes.io/name=zookeeper,app.kubernetes.io/instance=zookeeper,app.kubernetes.io/component=zookeeper\" -o jsonpath=\"{.items[0].metadata.name}\") kubectl exec -it $POD_NAME -- zkCli.sh To connect to your ZooKeeper server from outside the cluster execute the following commands: kubectl port-forward svc/zookeeper 2181:2181 & zkCli.sh 127.0.0.1:2181 WARNING: Rolling tag detected (bitnami/zookeeper:3.6.3), please note that it is strongly recommended to avoid using rolling tags in a production environment. +info https://docs.bitnami.com/containers/how-to/understand-rolling-tags-containers/ This will by default install a 3 nodes zookeeper ensemble. Wait until all the three pods are marked as ready: kubectl get pods NAME READY STATUS RESTARTS AGE zookeeper-0 1/1 Running 0 22h zookeeper-1 1/1 Running 0 4d22h zookeeper-2 1/1 Running 0 16m","title":"Install Zookeeper"},{"location":"deployment/deploy-k8s/#configuring-and-starting-hstreamdb","text":"Once all the zookeeper pods are ready, we're ready to start installing the HStreamDB cluster.","title":"Configuring and Starting HStreamDB"},{"location":"deployment/deploy-k8s/#fetching-the-k8s-specs","text":"git clone git@github.com:hstreamdb/hstream.git cd hstream/k8s","title":"Fetching The K8s Specs"},{"location":"deployment/deploy-k8s/#update-configuration","text":"If you used a different way to install zookeeper, make sure to update the zookeeper connection string in storage config file config.json and server service file hstream-server.yaml . It should look something like this: $ cat config.json | grep -A 2 zookeeper \"zookeeper\" : { \"zookeeper_uri\" : \"ip://zookeeper-0.zookeeper-headless:2181,zookeeper-1.zookeeper-headless:2181,zookeeper-2.zookeeper-headless:2181\" , \"timeout\" : \"30s\" } $ cat hstream-server.yaml | grep -A 1 zkuri - \"--zkuri\" - \"zookeeper-0.zookeeper-headless:2181,zookeeper-1.zookeeper-headless:2181,zookeeper-2.zookeeper-headless:2181\" Tips The zookeeper connection string in stotage config file and the service file can be different. But for normal scenario, they are the same. By default, this spec installs a 3 nodes HStream server cluster and 4 nodes storage cluster. If you want a bigger cluster, modify the hstream-server.yaml and logdevice-statefulset.yaml file, and increase the number of replicas to the number of nodes you want in the cluster. Also by default, we attach a 40GB persistent storage to the nodes, if you want more you can change that under the volumeClaimTemplates section.","title":"Update Configuration"},{"location":"deployment/deploy-k8s/#starting-the-cluster","text":"kubectl apply -k . When you run kubectl get pods , you should see something like this: NAME READY STATUS RESTARTS AGE hstream-server-deployment-765c84c489-94nqd 1/1 Running 0 6d18h hstream-server-deployment-765c84c489-jrm5p 1/1 Running 0 6d18h hstream-server-deployment-765c84c489-jxsjd 1/1 Running 0 6d18h logdevice-0 1/1 Running 0 6d18h logdevice-1 1/1 Running 0 6d18h logdevice-2 1/1 Running 0 6d18h logdevice-3 1/1 Running 0 6d18h logdevice-admin-server-deployment-5c5fb9f8fb-27jlk 1/1 Running 0 6d18h zookeeper-0 1/1 Running 0 6d22h zookeeper-1 1/1 Running 0 10d zookeeper-2 1/1 Running 0 6d","title":"Starting the Cluster"},{"location":"deployment/deploy-k8s/#bootstrapping-the-storage-cluster","text":"Once all the logdevice pods are running and ready, you'll need to bootstrap the cluster to enable all the nodes. To do that, run: kubectl run hadmin -it --rm --restart = Never --image = hstreamdb/hstream -- \\ hadmin --host logdevice-admin-server-service \\ nodes-config \\ bootstrap --metadata-replicate-across 'node:3' This will start a hadmin pod, that connects to the admin server and invokes the nodes-config bootstrap hadmin command and sets the metadata replication property of the cluster to be replicated across three different nodes. On success, you should see something like: Successfully bootstrapped the cluster pod \"hadmin\" deleted","title":"Bootstrapping the Storage Cluster"},{"location":"deployment/deploy-k8s/#managing-the-storage-cluster","text":"kubectl run hadmin -it --rm --restart = Never --image = hstreamdb/hstream -- bash Now you can run hadmin to manage the cluster: hadmin --help To check the state of the cluster, you can then run: hadmin --host logdevice-admin-server-service status +----+-------------+-------+---------------+ | ID | NAME | STATE | HEALTH STATUS | +----+-------------+-------+---------------+ | 0 | logdevice-0 | ALIVE | HEALTHY | | 1 | logdevice-1 | ALIVE | HEALTHY | | 2 | logdevice-2 | ALIVE | HEALTHY | | 3 | logdevice-3 | ALIVE | HEALTHY | +----+-------------+-------+---------------+ Took 2.567s","title":"Managing the Storage Cluster"},{"location":"develop/java-sdk/connect/","text":"Connect \u00b6 This page shows how to connect to HStreamDB using Java SDK. Prerequisites \u00b6 Make sure you have HStreamDB running and accessible. Example \u00b6 package io.hstream.example ; import io.hstream.HStreamClient ; public class ConnectExample { public static void main ( String [] args ) throws Exception { final String serviceUrl = \"localhost:6570\" ; HStreamClient client = HStreamClient . builder (). serviceUrl ( serviceUrl ). build (); System . out . println ( \"Connected\" ); client . close (); } }","title":"Connect"},{"location":"develop/java-sdk/connect/#connect","text":"This page shows how to connect to HStreamDB using Java SDK.","title":"Connect"},{"location":"develop/java-sdk/connect/#prerequisites","text":"Make sure you have HStreamDB running and accessible.","title":"Prerequisites"},{"location":"develop/java-sdk/connect/#example","text":"package io.hstream.example ; import io.hstream.HStreamClient ; public class ConnectExample { public static void main ( String [] args ) throws Exception { final String serviceUrl = \"localhost:6570\" ; HStreamClient client = HStreamClient . builder (). serviceUrl ( serviceUrl ). build (); System . out . println ( \"Connected\" ); client . close (); } }","title":"Example"},{"location":"develop/java-sdk/consume/","text":"Consume data \u00b6 This page shows how to consume data from HStreamDB using Java SDK. Prerequisites \u00b6 Make sure you have HStreamDB running and accessible. Consumer \u00b6 Before you can consume data, you first need to create a Consumer object using the HStreamClient.newConsumer() method: Consumer consumer = client . newConsumer () . subscription ( \"test_subscription\" ) . stream ( \"test_stream\" ) . maxPollRecords ( 100 ) . pollTimeoutMs ( 1000 ) . build (); A consumer must be associated with a subscription, and a subscription contains a stream. Once the consumer is created successfully, it can be used to continuously receive data from the subscribed stream. Receive Raw Records \u00b6 You can receive receive raw records using the Consumer.pollRawRecords() method: while ( true ) { List < ReceivedRawRecord > receivedRawRecords = consumer . pollRawRecords (); for ( ReceivedRawRecord receivedRawRecord : receivedRawRecords ) { System . out . println ( receivedRawRecord . getRecordId ()); } } Receive HRecords \u00b6 You can receive receive hrecords using the Consumer.pollHRecords() method: while ( true ) { List < ReceivedHRecord > receivedHRecords = consumer . pollHRecords (); for ( ReceivedHRecord receivedHRecord : receivedHRecords ) { System . out . println ( receivedHRecord . getRecordId ()); } }","title":"Consume Data"},{"location":"develop/java-sdk/consume/#consume-data","text":"This page shows how to consume data from HStreamDB using Java SDK.","title":"Consume data"},{"location":"develop/java-sdk/consume/#prerequisites","text":"Make sure you have HStreamDB running and accessible.","title":"Prerequisites"},{"location":"develop/java-sdk/consume/#consumer","text":"Before you can consume data, you first need to create a Consumer object using the HStreamClient.newConsumer() method: Consumer consumer = client . newConsumer () . subscription ( \"test_subscription\" ) . stream ( \"test_stream\" ) . maxPollRecords ( 100 ) . pollTimeoutMs ( 1000 ) . build (); A consumer must be associated with a subscription, and a subscription contains a stream. Once the consumer is created successfully, it can be used to continuously receive data from the subscribed stream.","title":"Consumer"},{"location":"develop/java-sdk/consume/#receive-raw-records","text":"You can receive receive raw records using the Consumer.pollRawRecords() method: while ( true ) { List < ReceivedRawRecord > receivedRawRecords = consumer . pollRawRecords (); for ( ReceivedRawRecord receivedRawRecord : receivedRawRecords ) { System . out . println ( receivedRawRecord . getRecordId ()); } }","title":"Receive Raw Records"},{"location":"develop/java-sdk/consume/#receive-hrecords","text":"You can receive receive hrecords using the Consumer.pollHRecords() method: while ( true ) { List < ReceivedHRecord > receivedHRecords = consumer . pollHRecords (); for ( ReceivedHRecord receivedHRecord : receivedHRecords ) { System . out . println ( receivedHRecord . getRecordId ()); } }","title":"Receive HRecords"},{"location":"develop/java-sdk/installation/","text":"Installation \u00b6 The HStreamDB Java SDK is published in Maven central, available at hstreamdb-java . Maven \u00b6 For Maven Users, the library can be included easily like this: <dependencies> <dependency> <groupId> io.hstream </groupId> <artifactId> hstreamdb-java </artifactId> <version> ${hstreamdbClient.version} </version> </dependency> </dependencies> Gradle \u00b6 For Gradle Users, the library can be included easily like this: compile group: 'io.hstreamdb' , name: 'hstreamdb-java' , version: \"${hstreamdbClientVersion}\"","title":"Installation"},{"location":"develop/java-sdk/installation/#installation","text":"The HStreamDB Java SDK is published in Maven central, available at hstreamdb-java .","title":"Installation"},{"location":"develop/java-sdk/installation/#maven","text":"For Maven Users, the library can be included easily like this: <dependencies> <dependency> <groupId> io.hstream </groupId> <artifactId> hstreamdb-java </artifactId> <version> ${hstreamdbClient.version} </version> </dependency> </dependencies>","title":"Maven"},{"location":"develop/java-sdk/installation/#gradle","text":"For Gradle Users, the library can be included easily like this: compile group: 'io.hstreamdb' , name: 'hstreamdb-java' , version: \"${hstreamdbClientVersion}\"","title":"Gradle"},{"location":"develop/java-sdk/query/","text":"Stream Processing with SQL \u00b6 This page shows how to processing stream data in HStreamDB with SQL using Java SDK. Prerequisites \u00b6 Make sure you have HStreamDB running and accessible. Execute Real-time Query on Stream \u00b6 You can execute a real-time query on stream using the HStreamClient.streamQuery() method: final String TEST_STREAM = \"test_stream\" ; Publisher < HRecord > publisher = client . streamQuery ( \"select * from \" + TEST_STREAM + \" where temperature > 30 emit changes;\" ); Observer < HRecord > observer = new Observer < HRecord > () { @Override public void onNext ( HRecord hrecord ) { System . out . println ( hrecord ); } @Override public void onError ( Throwable t ) { throw new RuntimeException ( t ); } @Override public void onCompleted () { } }; publisher . subscribe ( observer ); The HStreamClient.streamQuery() method return a Publisher object, and you need to provide an Observer object that contains your logic for processing the results returned by the query.","title":"Stream Processing with SQL"},{"location":"develop/java-sdk/query/#stream-processing-with-sql","text":"This page shows how to processing stream data in HStreamDB with SQL using Java SDK.","title":"Stream Processing with SQL"},{"location":"develop/java-sdk/query/#prerequisites","text":"Make sure you have HStreamDB running and accessible.","title":"Prerequisites"},{"location":"develop/java-sdk/query/#execute-real-time-query-on-stream","text":"You can execute a real-time query on stream using the HStreamClient.streamQuery() method: final String TEST_STREAM = \"test_stream\" ; Publisher < HRecord > publisher = client . streamQuery ( \"select * from \" + TEST_STREAM + \" where temperature > 30 emit changes;\" ); Observer < HRecord > observer = new Observer < HRecord > () { @Override public void onNext ( HRecord hrecord ) { System . out . println ( hrecord ); } @Override public void onError ( Throwable t ) { throw new RuntimeException ( t ); } @Override public void onCompleted () { } }; publisher . subscribe ( observer ); The HStreamClient.streamQuery() method return a Publisher object, and you need to provide an Observer object that contains your logic for processing the results returned by the query.","title":"Execute Real-time Query on Stream"},{"location":"develop/java-sdk/stream/","text":"Streams \u00b6 HStreamDB stores data in streams, and this page shows how to operate streams using Java SDK. Prerequisites \u00b6 Make sure you have HStreamDB running and accessible. Include following import statements: import io.hstream.HStreamClient ; import io.hstream.Stream ; Connect to a HStreamDB Instance \u00b6 First, you need to connect to a HStreaDB instance and get a HStreamClient oject. HStreamClient client = HStreamClient . builder (). serviceUrl ( \"localhost:6570).build(); Get a List of Streams \u00b6 You can get a list of the streams using the HStreamClient.listStreams() method: for ( Stream stream : client . listStreams ()) { System . out . println ( stream . getStreamName ()); } Create a New Stream \u00b6 You can create a new stream using the HStreamClient.createStream() method: client . createStream ( \"test_stream\" ); Delete a Stream \u00b6 You can delete a stream using the HStreamClient.deleteStream() method: client . deleteStream ( \"test_stream\" );","title":"Streams"},{"location":"develop/java-sdk/stream/#streams","text":"HStreamDB stores data in streams, and this page shows how to operate streams using Java SDK.","title":"Streams"},{"location":"develop/java-sdk/stream/#prerequisites","text":"Make sure you have HStreamDB running and accessible. Include following import statements: import io.hstream.HStreamClient ; import io.hstream.Stream ;","title":"Prerequisites"},{"location":"develop/java-sdk/stream/#connect-to-a-hstreamdb-instance","text":"First, you need to connect to a HStreaDB instance and get a HStreamClient oject. HStreamClient client = HStreamClient . builder (). serviceUrl ( \"localhost:6570).build();","title":"Connect to a HStreamDB Instance"},{"location":"develop/java-sdk/stream/#get-a-list-of-streams","text":"You can get a list of the streams using the HStreamClient.listStreams() method: for ( Stream stream : client . listStreams ()) { System . out . println ( stream . getStreamName ()); }","title":"Get a List of Streams"},{"location":"develop/java-sdk/stream/#create-a-new-stream","text":"You can create a new stream using the HStreamClient.createStream() method: client . createStream ( \"test_stream\" );","title":"Create a New Stream"},{"location":"develop/java-sdk/stream/#delete-a-stream","text":"You can delete a stream using the HStreamClient.deleteStream() method: client . deleteStream ( \"test_stream\" );","title":"Delete a Stream"},{"location":"develop/java-sdk/write/","text":"Write data \u00b6 This page shows how to write data into HStreamDB using Java SDK. Prerequisites \u00b6 Make sure you have HStreamDB running and accessible. Concepts \u00b6 You can write two types of data to streams in HStreamDB: raw record hstream record(HRecord) Raw Record \u00b6 Raw record represents arbitray binary data. You can save binary data to a stream, but please note that currently stream processing via sql ignores binary data, it now only processes HRecord type data. Of course, you can always get the binary data from the stream and process it yourself. HRecord \u00b6 You can think of an HRecord as a piece of JSON data\uff0c just like a document in some nosql databases. You can process hrecords directly in real time via sql statements. Producer \u00b6 Before you can write data, you first need to create a Producer object using the HStreamClient.newProducer() method: Producer producer = client . newProducer (). stream ( \"test_stream\" ). build (); A producer has some options, for now, let's just ignore them and use the default settings. Write Binary Data \u00b6 Write Binary Data Synchronously \u00b6 You can write binary data synchronously using the Producer.write() method: Random random = new Random (); byte [] rawRecord = new byte [ 100 ] ; random . nextBytes ( rawRecord ); RecordId recordId = producer . write ( rawRecord ); Write Binary Data Asynchronously \u00b6 You can write binary data asynchronously using the Producer.writeAsync() method: Random random = new Random (); byte [] rawRecord = new byte [ 100 ] ; random . nextBytes ( rawRecord ); CompletableFuture < RecordId > future = producer . writeAsync ( rawRecord ); Write HRecord \u00b6 Write HRecord Synchronously \u00b6 You can write hrecords synchronously using the Producer.write() method: HRecord hRecord = HRecord . newBuilder () . put ( \"key1\" , 10 ) . put ( \"key2\" , \"hello\" ) . put ( \"key3\" , true ) . build (); RecordId recordId = producer . write ( hRecord ); Write HRecord Asynchronously \u00b6 You can write hrecords asynchronously using the Producer.writeAsync() method: HRecord hRecord = HRecord . newBuilder () . put ( \"key1\" , 10 ) . put ( \"key2\" , \"hello\" ) . put ( \"key3\" , true ) . build (); CompletableFuture < RecordId > future = producer . write ( hRecord ); Buffered Writes (Preferred) \u00b6 When writing to HStreamDB, sending many small records limits throughput. To achieve higher thoughput, you can enable batch mode of Producer . Producer producer = client . newProducer () . stream ( \"test_stream\" ) . enableBatch () . recordCountLimit ( 100 ) . build (); Then you can still write data using the Producer.writeAsync() Random random = new Random (); final int count = 1000 ; CompletableFuture < RecordId >[] recordIdFutures = new CompletableFuture [ count ] ; for ( int i = 0 ; i < count ; ++ i ) { byte [] rawRecord = new byte [ 100 ] ; random . nextBytes ( rawRecord ); CompletableFuture < RecordId > future = producer . writeAsync ( rawRecord ); recordIdFutures [ i ] = future ; } Now the producer will first put the data submitted by the writeAsync method in an internal buffer and send it together to the HStreamDB server when the number reaches recordLimitCount , or you can call flush method manually at any time to flush the buffer. producer . flush (); Warnings \u00b6 Please do not write both binary data and hrecord in one stream.","title":"Write Data"},{"location":"develop/java-sdk/write/#write-data","text":"This page shows how to write data into HStreamDB using Java SDK.","title":"Write data"},{"location":"develop/java-sdk/write/#prerequisites","text":"Make sure you have HStreamDB running and accessible.","title":"Prerequisites"},{"location":"develop/java-sdk/write/#concepts","text":"You can write two types of data to streams in HStreamDB: raw record hstream record(HRecord)","title":"Concepts"},{"location":"develop/java-sdk/write/#raw-record","text":"Raw record represents arbitray binary data. You can save binary data to a stream, but please note that currently stream processing via sql ignores binary data, it now only processes HRecord type data. Of course, you can always get the binary data from the stream and process it yourself.","title":"Raw Record"},{"location":"develop/java-sdk/write/#hrecord","text":"You can think of an HRecord as a piece of JSON data\uff0c just like a document in some nosql databases. You can process hrecords directly in real time via sql statements.","title":"HRecord"},{"location":"develop/java-sdk/write/#producer","text":"Before you can write data, you first need to create a Producer object using the HStreamClient.newProducer() method: Producer producer = client . newProducer (). stream ( \"test_stream\" ). build (); A producer has some options, for now, let's just ignore them and use the default settings.","title":"Producer"},{"location":"develop/java-sdk/write/#write-binary-data","text":"","title":"Write Binary Data"},{"location":"develop/java-sdk/write/#write-binary-data-synchronously","text":"You can write binary data synchronously using the Producer.write() method: Random random = new Random (); byte [] rawRecord = new byte [ 100 ] ; random . nextBytes ( rawRecord ); RecordId recordId = producer . write ( rawRecord );","title":"Write Binary Data Synchronously"},{"location":"develop/java-sdk/write/#write-binary-data-asynchronously","text":"You can write binary data asynchronously using the Producer.writeAsync() method: Random random = new Random (); byte [] rawRecord = new byte [ 100 ] ; random . nextBytes ( rawRecord ); CompletableFuture < RecordId > future = producer . writeAsync ( rawRecord );","title":"Write Binary Data Asynchronously"},{"location":"develop/java-sdk/write/#write-hrecord","text":"","title":"Write HRecord"},{"location":"develop/java-sdk/write/#write-hrecord-synchronously","text":"You can write hrecords synchronously using the Producer.write() method: HRecord hRecord = HRecord . newBuilder () . put ( \"key1\" , 10 ) . put ( \"key2\" , \"hello\" ) . put ( \"key3\" , true ) . build (); RecordId recordId = producer . write ( hRecord );","title":"Write HRecord Synchronously"},{"location":"develop/java-sdk/write/#write-hrecord-asynchronously","text":"You can write hrecords asynchronously using the Producer.writeAsync() method: HRecord hRecord = HRecord . newBuilder () . put ( \"key1\" , 10 ) . put ( \"key2\" , \"hello\" ) . put ( \"key3\" , true ) . build (); CompletableFuture < RecordId > future = producer . write ( hRecord );","title":"Write HRecord Asynchronously"},{"location":"develop/java-sdk/write/#buffered-writes-preferred","text":"When writing to HStreamDB, sending many small records limits throughput. To achieve higher thoughput, you can enable batch mode of Producer . Producer producer = client . newProducer () . stream ( \"test_stream\" ) . enableBatch () . recordCountLimit ( 100 ) . build (); Then you can still write data using the Producer.writeAsync() Random random = new Random (); final int count = 1000 ; CompletableFuture < RecordId >[] recordIdFutures = new CompletableFuture [ count ] ; for ( int i = 0 ; i < count ; ++ i ) { byte [] rawRecord = new byte [ 100 ] ; random . nextBytes ( rawRecord ); CompletableFuture < RecordId > future = producer . writeAsync ( rawRecord ); recordIdFutures [ i ] = future ; } Now the producer will first put the data submitted by the writeAsync method in an internal buffer and send it together to the HStreamDB server when the number reaches recordLimitCount , or you can call flush method manually at any time to flush the buffer. producer . flush ();","title":"Buffered Writes (Preferred)"},{"location":"develop/java-sdk/write/#warnings","text":"Please do not write both binary data and hrecord in one stream.","title":"Warnings"},{"location":"overview/features/","text":"Features \u00b6 Note: The following features the milestone of HStreamDB version 1.0. Some features are under continuous development and not yet fully implemented in the current version. Please stay tuned. HStreamDB Functional architecture Streaming data processing via SQL \u00b6 HStreamDB has designed a complete processing solution based on event time. It supports basic filtering and conversion operations, aggregations by key, calculations based on various time windows, joining between data streams, and processing disordered and late messages to ensure the accuracy of calculation results. Simultaneously, the stream processing solution of HStream is highly extensible, and users can extend the interface according to their own needs. Materialized View \u00b6 HStreamDB will offer materialized view to support complex query and analysis operations on continuously updated data streams. The incremental computing engine updates the materialized view instantly according to the changes of data streams, and users can query the materialized view through SQL statements to get real-time data insights. Data Stream Management \u00b6 HStreamDB supports the creation and management of large data streams. The creation of a data stream is a very light-weight operation based on an optimized storage design. It is possible to maintain a stable read/write latency in the case of many concurrent reads and writes. Persistent storage \u00b6 HStreamDB provides low latency and reliable data stream storage. It ensures that written data messages are not lost and can be consumed repeatedly. HStreamDB replicates written data messages to multiple storage nodes for high availability and fault tolerance and supports dumping cold data to lower-cost storage services, such as object storage, distributed file storage, etc. This means the storage capacity can be infinitely scalable and achieve permanent storage of data. Data streams access and distribution \u00b6 Connector deals with access and distribution of HStreamDB data. They connect to various data systems, including MQTT Broker, MySQL, ElasticSearch, Redis, etc., facilitating integration with external data systems for users. Monitoring and O&M tools \u00b6 We will set up a web-based console with system dashboards and visual charts, enabling detailed monitoring of cluster machine status, system key indicators, etc., which make it more convenient for O&M staff to manage the cluster.","title":"Features"},{"location":"overview/features/#features","text":"Note: The following features the milestone of HStreamDB version 1.0. Some features are under continuous development and not yet fully implemented in the current version. Please stay tuned. HStreamDB Functional architecture","title":"Features"},{"location":"overview/features/#streaming-data-processing-via-sql","text":"HStreamDB has designed a complete processing solution based on event time. It supports basic filtering and conversion operations, aggregations by key, calculations based on various time windows, joining between data streams, and processing disordered and late messages to ensure the accuracy of calculation results. Simultaneously, the stream processing solution of HStream is highly extensible, and users can extend the interface according to their own needs.","title":"Streaming data processing via SQL"},{"location":"overview/features/#materialized-view","text":"HStreamDB will offer materialized view to support complex query and analysis operations on continuously updated data streams. The incremental computing engine updates the materialized view instantly according to the changes of data streams, and users can query the materialized view through SQL statements to get real-time data insights.","title":"Materialized View"},{"location":"overview/features/#data-stream-management","text":"HStreamDB supports the creation and management of large data streams. The creation of a data stream is a very light-weight operation based on an optimized storage design. It is possible to maintain a stable read/write latency in the case of many concurrent reads and writes.","title":"Data Stream Management"},{"location":"overview/features/#persistent-storage","text":"HStreamDB provides low latency and reliable data stream storage. It ensures that written data messages are not lost and can be consumed repeatedly. HStreamDB replicates written data messages to multiple storage nodes for high availability and fault tolerance and supports dumping cold data to lower-cost storage services, such as object storage, distributed file storage, etc. This means the storage capacity can be infinitely scalable and achieve permanent storage of data.","title":"Persistent storage"},{"location":"overview/features/#data-streams-access-and-distribution","text":"Connector deals with access and distribution of HStreamDB data. They connect to various data systems, including MQTT Broker, MySQL, ElasticSearch, Redis, etc., facilitating integration with external data systems for users.","title":"Data streams access and distribution"},{"location":"overview/features/#monitoring-and-om-tools","text":"We will set up a web-based console with system dashboards and visual charts, enabling detailed monitoring of cluster machine status, system key indicators, etc., which make it more convenient for O&M staff to manage the cluster.","title":"Monitoring and O&amp;M tools"},{"location":"overview/learning-resouces/","text":"TODO \u00b6","title":"Learning Resources"},{"location":"overview/learning-resouces/#todo","text":"","title":"TODO"},{"location":"overview/overview/","text":"HStream Streaming Database Overview \u00b6 HStreamDB is a streaming database designed for streaming data, with complete lifecycle management for accessing, storing, processing, and distributing large-scale real-time data streams . It uses standard SQL (and its stream extensions) as the primary interface language, with real-time as the main feature, and aims to simplify the operation and management of data streams and the development of real-time applications.","title":"HStream Streaming Database Overview"},{"location":"overview/overview/#hstream-streaming-database-overview","text":"HStreamDB is a streaming database designed for streaming data, with complete lifecycle management for accessing, storing, processing, and distributing large-scale real-time data streams . It uses standard SQL (and its stream extensions) as the primary interface language, with real-time as the main feature, and aims to simplify the operation and management of data streams and the development of real-time applications.","title":"HStream Streaming Database Overview"},{"location":"overview/release-notes/","text":"TODO \u00b6","title":"Release Notes"},{"location":"overview/release-notes/#todo","text":"","title":"TODO"},{"location":"overview/architecture/hstream-server/","text":"HStream Server \u00b6 HStream Server (HSQL), the core computation component of HStreamDB, is designed to be stateless. The primary responsibility of HSQL is to support client connection management, security authentication, SQL parsing and optimization, and operations for stream computation such as task creation, scheduling, execution, management, etc. HStream Server (HSQL) top-down layered structures \u00b6 Access Layer \u00b6 It is in charge of protocol processing, connection management, security authentication, and access control for client requests. SQL layer \u00b6 To perform most stream processing and real-time analysis tasks, clients interact with HStreamDB through SQL statements. This layer is mainly responsible for compiling these SQL statements into logical data flow diagrams. Like the classic database system model, it contains two core sub-components: SQL parser and SQL optimizer. The SQL parser deals with the lexical and syntactic analysis and the compilation from SQL statements to relational algebraic expressions; the SQL optimizer will optimize the generated execution plan based on various rules and contexts. Stream Layer \u00b6 Stream layer includes the implementation of various stream processing operators, the data structures and DSL to express data flow diagrams, and the support for user-defined functions as processing operators. So, it is responsible for selecting the corresponding operator and optimization to generate the executable data flow diagram. Runtime Layer \u00b6 It is the layer responsible for executing the computation task of data flow diagrams and returning the results. The main components of the layer include task scheduler, state manager, and execution optimizer. The schedule takes care of the tasks scheduling between available computation resources, such as multiple threads of a single process, multiple processors of a single machine, and multiple machines or containers of a distributed cluster.","title":"HStream Server (HSQL)"},{"location":"overview/architecture/hstream-server/#hstream-server","text":"HStream Server (HSQL), the core computation component of HStreamDB, is designed to be stateless. The primary responsibility of HSQL is to support client connection management, security authentication, SQL parsing and optimization, and operations for stream computation such as task creation, scheduling, execution, management, etc.","title":"HStream Server"},{"location":"overview/architecture/hstream-server/#hstream-server-hsql-top-down-layered-structures","text":"","title":"HStream Server (HSQL) top-down layered structures"},{"location":"overview/architecture/hstream-server/#access-layer","text":"It is in charge of protocol processing, connection management, security authentication, and access control for client requests.","title":"Access Layer"},{"location":"overview/architecture/hstream-server/#sql-layer","text":"To perform most stream processing and real-time analysis tasks, clients interact with HStreamDB through SQL statements. This layer is mainly responsible for compiling these SQL statements into logical data flow diagrams. Like the classic database system model, it contains two core sub-components: SQL parser and SQL optimizer. The SQL parser deals with the lexical and syntactic analysis and the compilation from SQL statements to relational algebraic expressions; the SQL optimizer will optimize the generated execution plan based on various rules and contexts.","title":"SQL layer"},{"location":"overview/architecture/hstream-server/#stream-layer","text":"Stream layer includes the implementation of various stream processing operators, the data structures and DSL to express data flow diagrams, and the support for user-defined functions as processing operators. So, it is responsible for selecting the corresponding operator and optimization to generate the executable data flow diagram.","title":"Stream Layer"},{"location":"overview/architecture/hstream-server/#runtime-layer","text":"It is the layer responsible for executing the computation task of data flow diagrams and returning the results. The main components of the layer include task scheduler, state manager, and execution optimizer. The schedule takes care of the tasks scheduling between available computation resources, such as multiple threads of a single process, multiple processors of a single machine, and multiple machines or containers of a distributed cluster.","title":"Runtime Layer"},{"location":"overview/architecture/hstream-store/","text":"HStream Storage (HStore) \u00b6 HStream Storage (HStore), the core storage component of HStreamDB, is a low-latency storage component explicitly designed for streaming data. It can store large-scale real-time data in a distributed and persistent manner and seamlessly interface with large-capacity secondary storage such as S3 through the Auto-Tiering mechanism to achieve unified storage of historical and real-time data. The core storage model of HStore is a logging model that fits with streaming data. Regard data stream as an infinitely growing log, the typical operations supported include appending and reading by batches. Also, since the data stream is immutable, it generally does not support update operations. HStream Storage (HStore) consists of following layer \u00b6 Streaming Data API layer \u00b6 This layer provides the core data stream management and read/write operations, including stream creation/deletion and writing to/consuming data in the stream. In the design of HStore, data streams are not stored as actual streams. Therefore, the creation of a stream is a very light-weight operation. There is no limit to the number of streams to be created in HStore. Besides, it supports concurrent writes to numerous data streams and still maintains a stable low latency. For the characteristics of data streams, HStore provides append operation to support fast data writing. While reading from stream data, it gives a subscription-based operation and pushes any new data written to the stream to the data consumer in real time. Replicator Layer \u00b6 This layer implements the strongly consistent replication based on an optimized Flexible Paxos consensus mechanism, ensuring the fault tolerance and high availability to data, and maximizes cluster availability through a non-deterministic data distribution policy. Moreover, it supports replication groups reconfiguration online to achieve seamless cluster data balancing and horizontal scaling. Tier1 Local Storage Layer \u00b6 The layer fulfilled local persistent storage needs of data based on the optimized RocksDB storage engine, which encapsulates the access interface of streaming data and can support low-latency writing and reading a large amount of data. Tier2 Offloader Layer \u00b6 This layer provides a unified interface encapsulation for various long-term storage systems, such as HDFS, AWS S3, etc. It supports automatic offloading of historical data to these secondary storage systems and can also be accessed through a unified streaming data interface.","title":"HStream Store (HStore)"},{"location":"overview/architecture/hstream-store/#hstream-storage-hstore","text":"HStream Storage (HStore), the core storage component of HStreamDB, is a low-latency storage component explicitly designed for streaming data. It can store large-scale real-time data in a distributed and persistent manner and seamlessly interface with large-capacity secondary storage such as S3 through the Auto-Tiering mechanism to achieve unified storage of historical and real-time data. The core storage model of HStore is a logging model that fits with streaming data. Regard data stream as an infinitely growing log, the typical operations supported include appending and reading by batches. Also, since the data stream is immutable, it generally does not support update operations.","title":"HStream Storage (HStore)"},{"location":"overview/architecture/hstream-store/#hstream-storage-hstore-consists-of-following-layer","text":"","title":"HStream Storage (HStore) consists of following layer"},{"location":"overview/architecture/hstream-store/#streaming-data-api-layer","text":"This layer provides the core data stream management and read/write operations, including stream creation/deletion and writing to/consuming data in the stream. In the design of HStore, data streams are not stored as actual streams. Therefore, the creation of a stream is a very light-weight operation. There is no limit to the number of streams to be created in HStore. Besides, it supports concurrent writes to numerous data streams and still maintains a stable low latency. For the characteristics of data streams, HStore provides append operation to support fast data writing. While reading from stream data, it gives a subscription-based operation and pushes any new data written to the stream to the data consumer in real time.","title":"Streaming Data API layer"},{"location":"overview/architecture/hstream-store/#replicator-layer","text":"This layer implements the strongly consistent replication based on an optimized Flexible Paxos consensus mechanism, ensuring the fault tolerance and high availability to data, and maximizes cluster availability through a non-deterministic data distribution policy. Moreover, it supports replication groups reconfiguration online to achieve seamless cluster data balancing and horizontal scaling.","title":"Replicator Layer"},{"location":"overview/architecture/hstream-store/#tier1-local-storage-layer","text":"The layer fulfilled local persistent storage needs of data based on the optimized RocksDB storage engine, which encapsulates the access interface of streaming data and can support low-latency writing and reading a large amount of data.","title":"Tier1 Local Storage Layer"},{"location":"overview/architecture/hstream-store/#tier2-offloader-layer","text":"This layer provides a unified interface encapsulation for various long-term storage systems, such as HDFS, AWS S3, etc. It supports automatic offloading of historical data to these secondary storage systems and can also be accessed through a unified streaming data interface.","title":"Tier2 Offloader Layer"},{"location":"overview/architecture/overview/","text":"HStream Database Overview \u00b6 HStreamDB is a streaming database designed for streaming data, with complete lifecycle management for accessing, storing, processing, and distributing large-scale real-time data streams . It uses standard SQL (and its stream extensions) as the primary interface language, with real-time as the main feature, and aims to simplify the operation and management of data streams and the development of real-time applications. Architecture \u00b6 The figure below shows the overall architecture of HStreamDB. A single HStreamDB node consists of two core components, HStream Server (HSQL) and HStream Storage (HStorage). And an HStream cluster consists of several peer-to-peer HStreamDB nodes. Clients can connect to any HStreamDB node in the cluster and perform stream processing and analysis through your familiar SQL language. HStreamDB Structure Overview","title":"Overview"},{"location":"overview/architecture/overview/#hstream-database-overview","text":"HStreamDB is a streaming database designed for streaming data, with complete lifecycle management for accessing, storing, processing, and distributing large-scale real-time data streams . It uses standard SQL (and its stream extensions) as the primary interface language, with real-time as the main feature, and aims to simplify the operation and management of data streams and the development of real-time applications.","title":"HStream Database Overview"},{"location":"overview/architecture/overview/#architecture","text":"The figure below shows the overall architecture of HStreamDB. A single HStreamDB node consists of two core components, HStream Server (HSQL) and HStream Storage (HStorage). And an HStream cluster consists of several peer-to-peer HStreamDB nodes. Clients can connect to any HStreamDB node in the cluster and perform stream processing and analysis through your familiar SQL language. HStreamDB Structure Overview","title":"Architecture"},{"location":"overview/concepts/stream/","text":"Stream and Streaming Data \u00b6 What is a Stream in HStreamDB? \u00b6 A stream is a flow of endless data assigned with a unique name. Streaming Data \u00b6 These endless data are known as Streaming data . Streaming data are data that are generated continuously and typically sent in data records with small sizes (order of Kilobytes). Stream \u00b6 Therefore, in a stream from HStreamDB, the default form of a single data is JSON. A flow indicates that the stream in HStreamDB is append-only, which is true. It makes no sense to delete something from a stream. Implicitly, every append operation comes with an LSN , meaning that every data in a stream is checkable and recoverable. LSN is the abbreviate for log sequence number , which is the combination of the epoch and the epoch sequence number. It is guaranteed to be monotonically increasing. Extra \u00b6 Examples of Streaming Data \u00b6 reference: streaming-data Sensors in transportation vehicles, industrial equipment, and farm machinery send data to a streaming application. The application monitors performance, detects any potential defects in advance, and places a spare part order automatically preventing equipment down time. A financial institution tracks changes in the stock market in real time, computes value-at-risk, and automatically rebalances portfolios based on stock price movements. A real-estate website tracks a subset of data from consumers\u2019 mobile devices and makes real-time property recommendations of properties to visit based on their geo-location. A solar power company has to maintain power throughput for its customers, or pay penalties. It implemented a streaming data application that monitors of all of panels in the field, and schedules service in real time, thereby minimizing the periods of low throughput from each panel and the associated penalty payouts. A media publisher streams billions of clickstream records from its online properties, aggregates and enriches the data with demographic information about users, and optimizes content placement on its site, delivering relevancy and better experience to its audience. An online gaming company collects streaming data about player-game interactions, and feeds the data into its gaming platform. It then analyzes the data in real-time, offers incentives and dynamic experiences to engage its players.","title":"Stream and Streaming Data"},{"location":"overview/concepts/stream/#stream-and-streaming-data","text":"","title":"Stream and Streaming Data"},{"location":"overview/concepts/stream/#what-is-a-stream-in-hstreamdb","text":"A stream is a flow of endless data assigned with a unique name.","title":"What is a Stream in HStreamDB?"},{"location":"overview/concepts/stream/#streaming-data","text":"These endless data are known as Streaming data . Streaming data are data that are generated continuously and typically sent in data records with small sizes (order of Kilobytes).","title":"Streaming Data"},{"location":"overview/concepts/stream/#stream","text":"Therefore, in a stream from HStreamDB, the default form of a single data is JSON. A flow indicates that the stream in HStreamDB is append-only, which is true. It makes no sense to delete something from a stream. Implicitly, every append operation comes with an LSN , meaning that every data in a stream is checkable and recoverable. LSN is the abbreviate for log sequence number , which is the combination of the epoch and the epoch sequence number. It is guaranteed to be monotonically increasing.","title":"Stream"},{"location":"overview/concepts/stream/#extra","text":"","title":"Extra"},{"location":"overview/concepts/stream/#examples-of-streaming-data","text":"reference: streaming-data Sensors in transportation vehicles, industrial equipment, and farm machinery send data to a streaming application. The application monitors performance, detects any potential defects in advance, and places a spare part order automatically preventing equipment down time. A financial institution tracks changes in the stock market in real time, computes value-at-risk, and automatically rebalances portfolios based on stock price movements. A real-estate website tracks a subset of data from consumers\u2019 mobile devices and makes real-time property recommendations of properties to visit based on their geo-location. A solar power company has to maintain power throughput for its customers, or pay penalties. It implemented a streaming data application that monitors of all of panels in the field, and schedules service in real time, thereby minimizing the periods of low throughput from each panel and the associated penalty payouts. A media publisher streams billions of clickstream records from its online properties, aggregates and enriches the data with demographic information about users, and optimizes content placement on its site, delivering relevancy and better experience to its audience. An online gaming company collects streaming data about player-game interactions, and feeds the data into its gaming platform. It then analyzes the data in real-time, offers incentives and dynamic experiences to engage its players.","title":"Examples of Streaming Data"},{"location":"sql/appendix/","text":"Appendix \u00b6 Data Types \u00b6 type examples Integer 1, -1, 1234567 Double 2.3, -3.56, 232.4 Bool TRUE, FALSE Date 2020-06-10 Time 11:18:30 String \"HStreamDB\" Keywords \u00b6 keyword description ABS absolute value ACOS arccosine ACOSH inverse hyperbolic cosine AND logical and operator AS stream or field name alias ASIN arcsine ASINH inverse hyperbolic sine ATAN arctangent ATANH inverse hyperbolic tangent AVG average function BETWEEN range operator, used with AND BY do something by certain conditions, used with GROUP or ORDER CEIL rounds a number UPWARDS to the nearest integer COS cosine COSH hyperbolic cosine COUNT count function CREATE create a stream / connector DATE prefix of date constant DAY interval unit DROP drop a stream EXP exponent FLOOR rounds a number DOWNWARDS to the nearest integer FORMAT specify the format of a stream FROM specify where to select data from GROUP group values by certain conditions, used with BY HAVING filter select values by a condition HOPPING hopping window INNER joining type, used with JOIN INSERT insert data into a stream, used with INTO INTERVAL prefix of interval constant INTO insert data into a stream, used with INSERT IS_ARRAY to determine if the given value is an array of values IS_BOOL to determine if the given value is a boolean IS_DATE to determine if the given value is a date value IS_FLOAT to determine if the given value is a float IS_INT to determine if the given value is an integer IS_MAP to determine if the given value is a map of values IS_NUM to determine if the given value is a number IS_STR to determine if the given value is a string IS_TIME to determine if the given value is a time value JOIN for joining two streams LEFT joining type, used with JOIN LEFT_TRIM trim spaces from the left end of a string LOG logarithm with base e LOG10 logarithm with base 10 LOG2 logarithm with base 2 MAX maximum function MIN minimum function MINUTE interval unit MONTH interval unit NOT logical not operator ON specify conditions, used with JOIN OR logical or operator ORDER sort values by certain conditions, used with BY OUTER joining type, used with JOIN REVERSE reverse a string RIGHT_TRIM trim spaces from the right end of a string ROUND rounds a number to the nearest integer SECOND interval unit SELECT query a stream SESSION session window SHOW show something to stdout SIN sine SINH hyperbolic sine SQRT square root STREAM specify a stream, used with CREATE STRLEN get the length of a string SUM sum function TAN tangent TANH hyperbolic tangent TIME prefix of the time constant TO_LOWER convert a string to lowercase TO_STR convert a value to string TO_UPPER convert a string to uppercase TRIM trim spaces from both ends of a string TUMBLING tumbling window VALUES specify inserted data, used with INSERT INTO WEEK interval unit WHERE filter selected values by a condition WITH specify properties when creating a stream WITHIN specify time window when joining two streams YEAR interval unit Operators \u00b6 operator description = equal to <> not equal to < less than > greater than <= less than or equal to >= greater than or equal to + addition - subtraction * multiplication . access field of a stream [] access item of a map or an array AND logical and operator OR logical or operator NOT logical not operator BETWEEN range operator Scalar Functions \u00b6 function description ABS absolute value ACOS arccosine ACOSH inverse hyperbolic cosine ASIN arcsine ASINH inverse hyperbolic sine ATAN arctangent ATANH inverse hyperbolic tangent CEIL rounds a number UPWARDS to the nearest integer COS cosine COSH hyperbolic cosine EXP exponent FLOOR rounds a number DOWNWARDS to the nearest integer IS_ARRAY to determine if the given value is an array of values IS_BOOL to determine if the given value is a boolean IS_DATE to determine if the given value is a date value IS_FLOAT to determine if the given value is a float IS_INT to determine if the given value is an integer IS_MAP to determine if the given value is a map of values IS_NUM to determine if the given value is a number IS_STR to determine if the given value is a string IS_TIME to determine if the given value is a time value LEFT_TRIM trim spaces from the left end of a string LOG logarithm with base e LOG10 logarithm with base 10 LOG2 logarithm with base 2 REVERSE reverse a string RIGHT_TRIM trim spaces from the right end of a string ROUND rounds a number to the nearest integer SIN sine SINH hyperbolic sine SQRT square root STRLEN get the length of a string TAN tangent TANH hyperbolic tangent TO_LOWER convert a string to lowercase TO_STR convert a value to string TO_UPPER convert a string to uppercase TRIM trim spaces from both ends of a string Aggregate Functions \u00b6 function description AVG average COUNT count MAX maximum MIN minimum SUM sum","title":"Appendix"},{"location":"sql/appendix/#appendix","text":"","title":"Appendix"},{"location":"sql/appendix/#data-types","text":"type examples Integer 1, -1, 1234567 Double 2.3, -3.56, 232.4 Bool TRUE, FALSE Date 2020-06-10 Time 11:18:30 String \"HStreamDB\"","title":"Data Types"},{"location":"sql/appendix/#keywords","text":"keyword description ABS absolute value ACOS arccosine ACOSH inverse hyperbolic cosine AND logical and operator AS stream or field name alias ASIN arcsine ASINH inverse hyperbolic sine ATAN arctangent ATANH inverse hyperbolic tangent AVG average function BETWEEN range operator, used with AND BY do something by certain conditions, used with GROUP or ORDER CEIL rounds a number UPWARDS to the nearest integer COS cosine COSH hyperbolic cosine COUNT count function CREATE create a stream / connector DATE prefix of date constant DAY interval unit DROP drop a stream EXP exponent FLOOR rounds a number DOWNWARDS to the nearest integer FORMAT specify the format of a stream FROM specify where to select data from GROUP group values by certain conditions, used with BY HAVING filter select values by a condition HOPPING hopping window INNER joining type, used with JOIN INSERT insert data into a stream, used with INTO INTERVAL prefix of interval constant INTO insert data into a stream, used with INSERT IS_ARRAY to determine if the given value is an array of values IS_BOOL to determine if the given value is a boolean IS_DATE to determine if the given value is a date value IS_FLOAT to determine if the given value is a float IS_INT to determine if the given value is an integer IS_MAP to determine if the given value is a map of values IS_NUM to determine if the given value is a number IS_STR to determine if the given value is a string IS_TIME to determine if the given value is a time value JOIN for joining two streams LEFT joining type, used with JOIN LEFT_TRIM trim spaces from the left end of a string LOG logarithm with base e LOG10 logarithm with base 10 LOG2 logarithm with base 2 MAX maximum function MIN minimum function MINUTE interval unit MONTH interval unit NOT logical not operator ON specify conditions, used with JOIN OR logical or operator ORDER sort values by certain conditions, used with BY OUTER joining type, used with JOIN REVERSE reverse a string RIGHT_TRIM trim spaces from the right end of a string ROUND rounds a number to the nearest integer SECOND interval unit SELECT query a stream SESSION session window SHOW show something to stdout SIN sine SINH hyperbolic sine SQRT square root STREAM specify a stream, used with CREATE STRLEN get the length of a string SUM sum function TAN tangent TANH hyperbolic tangent TIME prefix of the time constant TO_LOWER convert a string to lowercase TO_STR convert a value to string TO_UPPER convert a string to uppercase TRIM trim spaces from both ends of a string TUMBLING tumbling window VALUES specify inserted data, used with INSERT INTO WEEK interval unit WHERE filter selected values by a condition WITH specify properties when creating a stream WITHIN specify time window when joining two streams YEAR interval unit","title":"Keywords"},{"location":"sql/appendix/#operators","text":"operator description = equal to <> not equal to < less than > greater than <= less than or equal to >= greater than or equal to + addition - subtraction * multiplication . access field of a stream [] access item of a map or an array AND logical and operator OR logical or operator NOT logical not operator BETWEEN range operator","title":"Operators"},{"location":"sql/appendix/#scalar-functions","text":"function description ABS absolute value ACOS arccosine ACOSH inverse hyperbolic cosine ASIN arcsine ASINH inverse hyperbolic sine ATAN arctangent ATANH inverse hyperbolic tangent CEIL rounds a number UPWARDS to the nearest integer COS cosine COSH hyperbolic cosine EXP exponent FLOOR rounds a number DOWNWARDS to the nearest integer IS_ARRAY to determine if the given value is an array of values IS_BOOL to determine if the given value is a boolean IS_DATE to determine if the given value is a date value IS_FLOAT to determine if the given value is a float IS_INT to determine if the given value is an integer IS_MAP to determine if the given value is a map of values IS_NUM to determine if the given value is a number IS_STR to determine if the given value is a string IS_TIME to determine if the given value is a time value LEFT_TRIM trim spaces from the left end of a string LOG logarithm with base e LOG10 logarithm with base 10 LOG2 logarithm with base 2 REVERSE reverse a string RIGHT_TRIM trim spaces from the right end of a string ROUND rounds a number to the nearest integer SIN sine SINH hyperbolic sine SQRT square root STRLEN get the length of a string TAN tangent TANH hyperbolic tangent TO_LOWER convert a string to lowercase TO_STR convert a value to string TO_UPPER convert a string to uppercase TRIM trim spaces from both ends of a string","title":"Scalar Functions"},{"location":"sql/appendix/#aggregate-functions","text":"function description AVG average COUNT count MAX maximum MIN minimum SUM sum","title":"Aggregate Functions"},{"location":"sql/sql-overview/","text":"SQL Overview \u00b6 SQL is a domain-specific language used in programming and designed for managing data held in a database management system. A standard for the specification of SQL is maintained by the American National Standards Institute (ANSI). Also, there are many variants and extensions to SQL to express more specific programs. The SQL grammar of HStreamDB is based on a subset of SQL-92 with some extensions to support stream operations. Syntax \u00b6 SQL inputs are made up of a series of statements. Each statement is made up of a series of tokens and ends in a semicolon ( ; ). A token can be a keyword argument, an identifier, a literal, an operator, or a special character. The details of the rules can be found in the BNFC grammar file . Normally, tokens are separated by whitespace. The following examples are syntactically valid SQL statements: SELECT * FROM my_stream ; CREATE STREAM abnormal_weather AS SELECT * FROM weather WHERE temperature > 30 AND humidity > 80 WITH ( REPLICATE = 3 ); INSERT INTO weather ( cityId , temperature , humidity ) VALUES ( 11254469 , 12 , 65 ); Keywords \u00b6 Some tokens such as SELECT , INSERT and WHERE are reserved keywords , which have specific meanings in SQL syntax. Keywords are case insensitive, which means that SELECT and select are equivalent. A keyword can not be used as an identifier. For a complete list of keywords, see the appendix . Identifiers \u00b6 Identifiers are tokens that represent user-defined objects such as streams, fields, and other ones. For example, my_stream can be used as a stream name, and temperature can represent a field in the stream. By now, identifiers only support C-style naming rules. It means that an identifier name can only have letters (both uppercase and lowercase letters), digits, and the underscore. Besides, the first letter of an identifier should be either a letter or an underscore. By now, identifiers are case sensitive, which means that my_stream and MY_STREAM are different identifiers. Literals (Constants) \u00b6 Literals are objects with known values before being executed. There are six types of constants: integers, floats, strings, dates, time, and intervals so far. Integers \u00b6 Integers are in the form of digits , where digits are one or more single-digit integers (0 through 9). Negatives such as -1 are also supported. Note that scientific notation is not supported yet . Floats \u00b6 Floats are in the form of digits . digits . Negative floats such as -11.514 are supported. Note that scientific notation is not supported yet . Forms such as 1. and .99 are not supported yet . Strings \u00b6 Strings are arbitrary character series surrounded by double quotes ( \" ), such as \"JSON\" . Dates \u00b6 Dates represent a date exact to a day in the form of DATE <year>-<month>-<day> , where <year> , <month> and <day> are all integer constants. Note that the leading DATE should not be omitted. Example: DATE 2021-01-02 Time \u00b6 Time constants represent time exact to a second in the form of TIME <hour>-<minute>-<second> , where <hour> , <minute> and <second> are all integer constants. Note that the leading TIME should not be omitted. Example: TIME 11:45:14 Intervals \u00b6 Intervals represent a time section in the form of INTERVAL <num> <time_unit> , where <num> is an integer constant and <time_unit> is one of YEAR , MONTH , WEEK , DAY , MINUTE and SECOND . Note that the leading INTERVAL should not be omitted. Example: INTERVAL 5 SECOND Operators and Functions \u00b6 Functions are special keywords that mean some computation, such as SUM and MIN . And operators are infix functions composed of special characters, such as >= and <> . For a complete list of functions and operators, see the appendix . Special characters \u00b6 There are some special characters in the SQL syntax with particular meanings: Parentheses ( () ) are used outside an expression for controlling the order of evaluation or specifying a function application. Brackets ( [] ) are used with maps and arrays for accessing their substructures, such as some_map[temp] and some_array[1] . Note that it is not supported yet . Commas ( , ) are used for delineating a list of objects. The semicolons ( ; ) represent the end of a SQL statement. The asterisk ( * ) represents \"all fields\", such as SELECT * FROM my_stream; . The period ( . ) is used for accessing a field in a stream, such as my_stream.humidity . Comments \u00b6 A single-line comment begins with // : // This is a comment Also, C-style multi-line comments are supported: /* This is another comment */","title":"HStreamDB SQL overview"},{"location":"sql/sql-overview/#sql-overview","text":"SQL is a domain-specific language used in programming and designed for managing data held in a database management system. A standard for the specification of SQL is maintained by the American National Standards Institute (ANSI). Also, there are many variants and extensions to SQL to express more specific programs. The SQL grammar of HStreamDB is based on a subset of SQL-92 with some extensions to support stream operations.","title":"SQL Overview"},{"location":"sql/sql-overview/#syntax","text":"SQL inputs are made up of a series of statements. Each statement is made up of a series of tokens and ends in a semicolon ( ; ). A token can be a keyword argument, an identifier, a literal, an operator, or a special character. The details of the rules can be found in the BNFC grammar file . Normally, tokens are separated by whitespace. The following examples are syntactically valid SQL statements: SELECT * FROM my_stream ; CREATE STREAM abnormal_weather AS SELECT * FROM weather WHERE temperature > 30 AND humidity > 80 WITH ( REPLICATE = 3 ); INSERT INTO weather ( cityId , temperature , humidity ) VALUES ( 11254469 , 12 , 65 );","title":"Syntax"},{"location":"sql/sql-overview/#keywords","text":"Some tokens such as SELECT , INSERT and WHERE are reserved keywords , which have specific meanings in SQL syntax. Keywords are case insensitive, which means that SELECT and select are equivalent. A keyword can not be used as an identifier. For a complete list of keywords, see the appendix .","title":"Keywords"},{"location":"sql/sql-overview/#identifiers","text":"Identifiers are tokens that represent user-defined objects such as streams, fields, and other ones. For example, my_stream can be used as a stream name, and temperature can represent a field in the stream. By now, identifiers only support C-style naming rules. It means that an identifier name can only have letters (both uppercase and lowercase letters), digits, and the underscore. Besides, the first letter of an identifier should be either a letter or an underscore. By now, identifiers are case sensitive, which means that my_stream and MY_STREAM are different identifiers.","title":"Identifiers"},{"location":"sql/sql-overview/#literals-constants","text":"Literals are objects with known values before being executed. There are six types of constants: integers, floats, strings, dates, time, and intervals so far.","title":"Literals (Constants)"},{"location":"sql/sql-overview/#integers","text":"Integers are in the form of digits , where digits are one or more single-digit integers (0 through 9). Negatives such as -1 are also supported. Note that scientific notation is not supported yet .","title":"Integers"},{"location":"sql/sql-overview/#floats","text":"Floats are in the form of digits . digits . Negative floats such as -11.514 are supported. Note that scientific notation is not supported yet . Forms such as 1. and .99 are not supported yet .","title":"Floats"},{"location":"sql/sql-overview/#strings","text":"Strings are arbitrary character series surrounded by double quotes ( \" ), such as \"JSON\" .","title":"Strings"},{"location":"sql/sql-overview/#dates","text":"Dates represent a date exact to a day in the form of DATE <year>-<month>-<day> , where <year> , <month> and <day> are all integer constants. Note that the leading DATE should not be omitted. Example: DATE 2021-01-02","title":"Dates"},{"location":"sql/sql-overview/#time","text":"Time constants represent time exact to a second in the form of TIME <hour>-<minute>-<second> , where <hour> , <minute> and <second> are all integer constants. Note that the leading TIME should not be omitted. Example: TIME 11:45:14","title":"Time"},{"location":"sql/sql-overview/#intervals","text":"Intervals represent a time section in the form of INTERVAL <num> <time_unit> , where <num> is an integer constant and <time_unit> is one of YEAR , MONTH , WEEK , DAY , MINUTE and SECOND . Note that the leading INTERVAL should not be omitted. Example: INTERVAL 5 SECOND","title":"Intervals"},{"location":"sql/sql-overview/#operators-and-functions","text":"Functions are special keywords that mean some computation, such as SUM and MIN . And operators are infix functions composed of special characters, such as >= and <> . For a complete list of functions and operators, see the appendix .","title":"Operators and Functions"},{"location":"sql/sql-overview/#special-characters","text":"There are some special characters in the SQL syntax with particular meanings: Parentheses ( () ) are used outside an expression for controlling the order of evaluation or specifying a function application. Brackets ( [] ) are used with maps and arrays for accessing their substructures, such as some_map[temp] and some_array[1] . Note that it is not supported yet . Commas ( , ) are used for delineating a list of objects. The semicolons ( ; ) represent the end of a SQL statement. The asterisk ( * ) represents \"all fields\", such as SELECT * FROM my_stream; . The period ( . ) is used for accessing a field in a stream, such as my_stream.humidity .","title":"Special characters"},{"location":"sql/sql-overview/#comments","text":"A single-line comment begins with // : // This is a comment Also, C-style multi-line comments are supported: /* This is another comment */","title":"Comments"},{"location":"sql/sql-quick-reference/","text":"SQL quick reference \u00b6 CREATE STREAM \u00b6 Create a new HStreamDB stream with the stream name given. An exception will be thrown if the stream is already created. See CREATE STREAM . CREATE STREAM stream_name [ AS select_query ] [ WITH ( stream_option [, ...])]; CREATE VIEW \u00b6 Create a new view with the view name given. A view is a physical object like a stream and it is updated with time. An exception will be thrown if the view is already created. The name of a view can either be the same as a stream. See CREATE VIEW . CREATE VIEW view_name AS select_query ; CREATE CONNECTOR \u00b6 Create a new connector for fetching data from or writing data to an external system with the connector name given. A connector can be either a source or a sink one. Note that source connector is not supported yet . When creating a connector, its type and its bound stream must be specified in the WITH clause. There can be other options such as database name, user name and password. There can be an optional IF NOT EXIST config to only create the given connector if it does not exist. See CREATE CONNECTOR . CREATE < SOURCE | SINK > CONNECTOR connector_name [ IF NOT EXIST ] WITH ( connector_option [, ...]); Keep an eye on the status of the connectors by using SHOW CONNECTORS ; One of the following states is assigned to the connectors: state description Creating The server has started to process the request Created The connection has been established but it has not started to process the data CreationAbort The process of creating the connection failed and it is frozon Running The connector is ready to process requests ExecutionAbort The connector failed to execute a SQL statement and it is frozen Terminate The connector is frozen by a user request Please wait for it to finish setting up if the state of the connector is Creating or Created . You can restart an aborted or terminated connector (in the future). You may also abandon connectors by using DROP connector_name ; SELECT (from streams) \u00b6 Continuously get records from the stream(s) specified as streaming data flows in. It is usually used in an interactive CLI to monitor real-time changes of data. Note that the query writes these records to a random-named stream. See SELECT (Stream) . SELECT <* | expression [ AS field_alias ] [, ...] > FROM stream_name_1 [ join_type JOIN stream_name_2 WITHIN ( some_interval ) ON stream_name_1 . field_1 = stream_name_2 . field_2 ] [ WHERE search_condition ] [ GROUP BY field_name [, window_type ] ] EMIT CHANGES ; SELECT (from views) \u00b6 Get a record from the specified view. The fields to get have to be already in the view. It produces one or zero static records and costs little time. See Select (View) . SELECT <* | expression [ AS field_alias ] [, ...] > FROM view_name WHERE field_name = value_expression ; INSERT \u00b6 Insert data into the specified stream. It can be a data record, a JSON value or binary data. See INSERT . INSERT INTO stream_name ( field_name [, ...]) VALUES ( field_value [, ...]); INSERT INTO stream_name VALUES 'json_value' ; INSERT INTO stream_name VALUES \"binary_value\" ; DROP \u00b6 Delete a given connector, stream or view. There can be an optional IF EXISTS config to only delete the given category if it exists. DROP CONNECTOR connector_name [ IF EXISTS ]; DROP STREAM stream_name [ IF EXISTS ]; DROP VIEW view_name [ IF EXISTS ]; SHOW \u00b6 Show the information of all streams, queries, views or connectors. SHOW STREAMS ; SHOW QUERIES ; SHOW VIEWS ; SHOW CONNECTORS ;","title":"HStreamDB SQL quick reference"},{"location":"sql/sql-quick-reference/#sql-quick-reference","text":"","title":"SQL quick reference"},{"location":"sql/sql-quick-reference/#create-stream","text":"Create a new HStreamDB stream with the stream name given. An exception will be thrown if the stream is already created. See CREATE STREAM . CREATE STREAM stream_name [ AS select_query ] [ WITH ( stream_option [, ...])];","title":"CREATE STREAM"},{"location":"sql/sql-quick-reference/#create-view","text":"Create a new view with the view name given. A view is a physical object like a stream and it is updated with time. An exception will be thrown if the view is already created. The name of a view can either be the same as a stream. See CREATE VIEW . CREATE VIEW view_name AS select_query ;","title":"CREATE VIEW"},{"location":"sql/sql-quick-reference/#create-connector","text":"Create a new connector for fetching data from or writing data to an external system with the connector name given. A connector can be either a source or a sink one. Note that source connector is not supported yet . When creating a connector, its type and its bound stream must be specified in the WITH clause. There can be other options such as database name, user name and password. There can be an optional IF NOT EXIST config to only create the given connector if it does not exist. See CREATE CONNECTOR . CREATE < SOURCE | SINK > CONNECTOR connector_name [ IF NOT EXIST ] WITH ( connector_option [, ...]); Keep an eye on the status of the connectors by using SHOW CONNECTORS ; One of the following states is assigned to the connectors: state description Creating The server has started to process the request Created The connection has been established but it has not started to process the data CreationAbort The process of creating the connection failed and it is frozon Running The connector is ready to process requests ExecutionAbort The connector failed to execute a SQL statement and it is frozen Terminate The connector is frozen by a user request Please wait for it to finish setting up if the state of the connector is Creating or Created . You can restart an aborted or terminated connector (in the future). You may also abandon connectors by using DROP connector_name ;","title":"CREATE CONNECTOR"},{"location":"sql/sql-quick-reference/#select-from-streams","text":"Continuously get records from the stream(s) specified as streaming data flows in. It is usually used in an interactive CLI to monitor real-time changes of data. Note that the query writes these records to a random-named stream. See SELECT (Stream) . SELECT <* | expression [ AS field_alias ] [, ...] > FROM stream_name_1 [ join_type JOIN stream_name_2 WITHIN ( some_interval ) ON stream_name_1 . field_1 = stream_name_2 . field_2 ] [ WHERE search_condition ] [ GROUP BY field_name [, window_type ] ] EMIT CHANGES ;","title":"SELECT (from streams)"},{"location":"sql/sql-quick-reference/#select-from-views","text":"Get a record from the specified view. The fields to get have to be already in the view. It produces one or zero static records and costs little time. See Select (View) . SELECT <* | expression [ AS field_alias ] [, ...] > FROM view_name WHERE field_name = value_expression ;","title":"SELECT (from views)"},{"location":"sql/sql-quick-reference/#insert","text":"Insert data into the specified stream. It can be a data record, a JSON value or binary data. See INSERT . INSERT INTO stream_name ( field_name [, ...]) VALUES ( field_value [, ...]); INSERT INTO stream_name VALUES 'json_value' ; INSERT INTO stream_name VALUES \"binary_value\" ;","title":"INSERT"},{"location":"sql/sql-quick-reference/#drop","text":"Delete a given connector, stream or view. There can be an optional IF EXISTS config to only delete the given category if it exists. DROP CONNECTOR connector_name [ IF EXISTS ]; DROP STREAM stream_name [ IF EXISTS ]; DROP VIEW view_name [ IF EXISTS ];","title":"DROP"},{"location":"sql/sql-quick-reference/#show","text":"Show the information of all streams, queries, views or connectors. SHOW STREAMS ; SHOW QUERIES ; SHOW VIEWS ; SHOW CONNECTORS ;","title":"SHOW"},{"location":"sql/statements/create-connector/","text":"CREATE CONNECTOR \u00b6 Create a new connector for fetching data from or writing data to an external system. A connector can be either a source or a sink one. Note that source connector is not supported yet . Synopsis \u00b6 CREATE < SOURCE | SINK > CONNECTOR connector_name [ IF NOT EXIST ] WITH ( connector_option [, ...]); Notes \u00b6 connector_name is a valid identifier. There is an optional IF NOT EXIST configuration to create a connector only if the connector with the same name does not exist. There is are some connector options in the WITH clause separated by commas. TYPE and STREAM are required to specify the type of a connector and which stream it fetches data from/writes data to. For details, see the following table. TYPE Option Description Default Value mysql host Host of MySQL server \"127.0.0.1\" port Port of MySQL server 3306 username Username to login MySQL \"root\" password Password to login MySQL \"password\" database Database name to store data from HStreamDB \"mysql\" clickhouse host Host of ClickHouse server \"127.0.0.1\" port Port of ClickHouse server 9000 username Username to login ClickHouse \"default\" password Password to login ClickHouse \"\" database Database name to store data from HStreamDB \"default\" Examples \u00b6 CREATE SINK CONNECTOR mysql_conn WITH ( TYPE = mysql , STREAM = foo , host = \"127.0.0.1\" );","title":"CREATE CONNECTOR"},{"location":"sql/statements/create-connector/#create-connector","text":"Create a new connector for fetching data from or writing data to an external system. A connector can be either a source or a sink one. Note that source connector is not supported yet .","title":"CREATE CONNECTOR"},{"location":"sql/statements/create-connector/#synopsis","text":"CREATE < SOURCE | SINK > CONNECTOR connector_name [ IF NOT EXIST ] WITH ( connector_option [, ...]);","title":"Synopsis"},{"location":"sql/statements/create-connector/#notes","text":"connector_name is a valid identifier. There is an optional IF NOT EXIST configuration to create a connector only if the connector with the same name does not exist. There is are some connector options in the WITH clause separated by commas. TYPE and STREAM are required to specify the type of a connector and which stream it fetches data from/writes data to. For details, see the following table. TYPE Option Description Default Value mysql host Host of MySQL server \"127.0.0.1\" port Port of MySQL server 3306 username Username to login MySQL \"root\" password Password to login MySQL \"password\" database Database name to store data from HStreamDB \"mysql\" clickhouse host Host of ClickHouse server \"127.0.0.1\" port Port of ClickHouse server 9000 username Username to login ClickHouse \"default\" password Password to login ClickHouse \"\" database Database name to store data from HStreamDB \"default\"","title":"Notes"},{"location":"sql/statements/create-connector/#examples","text":"CREATE SINK CONNECTOR mysql_conn WITH ( TYPE = mysql , STREAM = foo , host = \"127.0.0.1\" );","title":"Examples"},{"location":"sql/statements/create-stream/","text":"CREATE STREAM \u00b6 Create a new hstream stream with the given name. An exception will be thrown if a stream with the same name already exists. Synopsis \u00b6 CREATE STREAM stream_name [ AS select_query ] WITH ( REPLICATE = INT ); Notes \u00b6 stream_name is a valid identifier. select_query is an optional SELECT (Stream) query. For more information, see SELECT section. When <select_query> is specified, the created stream will be filled with records from the SELECT query continuously. Otherwise, the stream will only be created and kept empty. WITH clause contains some stream options. Only REPLICATE option is supported now, which represents the replication factor of the stream. If it is not specified, the replication factor will be set to default value. Examples \u00b6 CREATE STREAM foo ; CREATE STREAM weather WITH ( FORMAT = \"JSON\" ); CREATE STREAM abnormal_weather AS SELECT * FROM weather WHERE temperature > 30 AND humidity > 80 EMIT CHANGES ;","title":"CREATE STREAM"},{"location":"sql/statements/create-stream/#create-stream","text":"Create a new hstream stream with the given name. An exception will be thrown if a stream with the same name already exists.","title":"CREATE STREAM"},{"location":"sql/statements/create-stream/#synopsis","text":"CREATE STREAM stream_name [ AS select_query ] WITH ( REPLICATE = INT );","title":"Synopsis"},{"location":"sql/statements/create-stream/#notes","text":"stream_name is a valid identifier. select_query is an optional SELECT (Stream) query. For more information, see SELECT section. When <select_query> is specified, the created stream will be filled with records from the SELECT query continuously. Otherwise, the stream will only be created and kept empty. WITH clause contains some stream options. Only REPLICATE option is supported now, which represents the replication factor of the stream. If it is not specified, the replication factor will be set to default value.","title":"Notes"},{"location":"sql/statements/create-stream/#examples","text":"CREATE STREAM foo ; CREATE STREAM weather WITH ( FORMAT = \"JSON\" ); CREATE STREAM abnormal_weather AS SELECT * FROM weather WHERE temperature > 30 AND humidity > 80 EMIT CHANGES ;","title":"Examples"},{"location":"sql/statements/create-view/","text":"CREATE VIEW \u00b6 Create a new hstream view with the given name. An exception will be thrown if a view or stream with the same name already exists. Synopsis \u00b6 CREATE VIEW view_name AS select_query ; Notes \u00b6 view_name is a valid identifier. select_query is a valid SELECT (Stream) query. For more information, see SELECT section. It has to contains at least one aggregate function and a GROUP BY clause. Examples \u00b6 CREATE VIEW foo AS SELECT a , SUM ( a ), COUNT ( * ) FROM bar GROUP BY b EMIT CHANGES ;","title":"CREATE VIEW"},{"location":"sql/statements/create-view/#create-view","text":"Create a new hstream view with the given name. An exception will be thrown if a view or stream with the same name already exists.","title":"CREATE VIEW"},{"location":"sql/statements/create-view/#synopsis","text":"CREATE VIEW view_name AS select_query ;","title":"Synopsis"},{"location":"sql/statements/create-view/#notes","text":"view_name is a valid identifier. select_query is a valid SELECT (Stream) query. For more information, see SELECT section. It has to contains at least one aggregate function and a GROUP BY clause.","title":"Notes"},{"location":"sql/statements/create-view/#examples","text":"CREATE VIEW foo AS SELECT a , SUM ( a ), COUNT ( * ) FROM bar GROUP BY b EMIT CHANGES ;","title":"Examples"},{"location":"sql/statements/insert/","text":"INSERT \u00b6 Insert a record into specified stream. Synopsis \u00b6 INSERT INTO stream_name ( field_name [, ...]) VALUES ( field_value [, ...]); INSERT INTO stream_name VALUES 'json_value' ; INSERT INTO stream_name VALUES \"binary_value\" ; Notes \u00b6 field_value represents the value of corresponding field, which is a constant . The correspondence between field type and inserted value is maintained by users themselves. json_value should be a valid JSON expression. And when inserting a JSON value, remember to put ' s around it. binary_value can be any value in the form of a string. It will not be processed by HStreamDB and can only be fetched by certain client API. Remember to put \" s around it. Examples \u00b6 INSERT INTO weather ( cityId , temperature , humidity ) VALUES ( 11254469 , 12 , 65 ); INSERT INTO foo VALUES '{\"a\": 1, \"b\": \"abc\"}' ; INSERT INTO bar VALUES \"some binary value \\x01\\x02\\x03\" ;","title":"INSERT"},{"location":"sql/statements/insert/#insert","text":"Insert a record into specified stream.","title":"INSERT"},{"location":"sql/statements/insert/#synopsis","text":"INSERT INTO stream_name ( field_name [, ...]) VALUES ( field_value [, ...]); INSERT INTO stream_name VALUES 'json_value' ; INSERT INTO stream_name VALUES \"binary_value\" ;","title":"Synopsis"},{"location":"sql/statements/insert/#notes","text":"field_value represents the value of corresponding field, which is a constant . The correspondence between field type and inserted value is maintained by users themselves. json_value should be a valid JSON expression. And when inserting a JSON value, remember to put ' s around it. binary_value can be any value in the form of a string. It will not be processed by HStreamDB and can only be fetched by certain client API. Remember to put \" s around it.","title":"Notes"},{"location":"sql/statements/insert/#examples","text":"INSERT INTO weather ( cityId , temperature , humidity ) VALUES ( 11254469 , 12 , 65 ); INSERT INTO foo VALUES '{\"a\": 1, \"b\": \"abc\"}' ; INSERT INTO bar VALUES \"some binary value \\x01\\x02\\x03\" ;","title":"Examples"},{"location":"sql/statements/select-stream/","text":"SELECT (Stream) \u00b6 Continuously pulls records from the stream(s) specified. It is usually used in an interactive CLI to monitor realtime changes of data. Note that the query writes records to a random-named stream which is hidden from users. Synopsis \u00b6 SELECT <* | expression [ AS field_alias ] [, ...] > FROM stream_name_1 [ join_type JOIN stream_name_2 WITHIN ( some_interval ) ON stream_name_1 . field_1 = stream_name_2 . field_2 ] [ WHERE search_condition ] [ GROUP BY field_name [, window_type ] ] EMIT CHANGES ; Notes \u00b6 expression can be a field name, a constant, or their association, such as temperature , weather.humidity , 114514 , 1 + 2 and SUM(productions) . some_interval represents a period of time. See Intervals . join_type specifies the type of joining operation. Only INNER is supported yet. window_type specifies the type of time window: window_type ::= TUMBLING some_interval | HOPPING some_interval some_interval | SESSION some_interval search_condition is actually a boolean expression: search_condition ::= [NOT] predicate [ <AND | OR> predicate [, ...] ] predicate ::= expression comp_op expression comp_op ::= = | <> | > | < | >= | <= Examples \u00b6 A simple query: SELECT * FROM my_stream EMIT CHANGES ; Filtering rows: SELECT temperature , humidity FROM weather WHERE temperature > 10 AND humidity < 75 EMIT CHANGES ; Joining streams: SELECT stream1 . temperature , stream2 . humidity FROM stream1 INNER JOIN stream2 WITHIN ( INTERVAL 5 SECOND ) ON stream1 . humidity = stream2 . humidity EMIT CHANGES ; Grouping records: SELECT COUNT ( * ) FROM weather GROUP BY cityId , TUMBLING ( INTERVAL 10 SECOND ) EMIT CHANGES ;","title":"SELECT (Stream)"},{"location":"sql/statements/select-stream/#select-stream","text":"Continuously pulls records from the stream(s) specified. It is usually used in an interactive CLI to monitor realtime changes of data. Note that the query writes records to a random-named stream which is hidden from users.","title":"SELECT (Stream)"},{"location":"sql/statements/select-stream/#synopsis","text":"SELECT <* | expression [ AS field_alias ] [, ...] > FROM stream_name_1 [ join_type JOIN stream_name_2 WITHIN ( some_interval ) ON stream_name_1 . field_1 = stream_name_2 . field_2 ] [ WHERE search_condition ] [ GROUP BY field_name [, window_type ] ] EMIT CHANGES ;","title":"Synopsis"},{"location":"sql/statements/select-stream/#notes","text":"expression can be a field name, a constant, or their association, such as temperature , weather.humidity , 114514 , 1 + 2 and SUM(productions) . some_interval represents a period of time. See Intervals . join_type specifies the type of joining operation. Only INNER is supported yet. window_type specifies the type of time window: window_type ::= TUMBLING some_interval | HOPPING some_interval some_interval | SESSION some_interval search_condition is actually a boolean expression: search_condition ::= [NOT] predicate [ <AND | OR> predicate [, ...] ] predicate ::= expression comp_op expression comp_op ::= = | <> | > | < | >= | <=","title":"Notes"},{"location":"sql/statements/select-stream/#examples","text":"A simple query: SELECT * FROM my_stream EMIT CHANGES ; Filtering rows: SELECT temperature , humidity FROM weather WHERE temperature > 10 AND humidity < 75 EMIT CHANGES ; Joining streams: SELECT stream1 . temperature , stream2 . humidity FROM stream1 INNER JOIN stream2 WITHIN ( INTERVAL 5 SECOND ) ON stream1 . humidity = stream2 . humidity EMIT CHANGES ; Grouping records: SELECT COUNT ( * ) FROM weather GROUP BY cityId , TUMBLING ( INTERVAL 10 SECOND ) EMIT CHANGES ;","title":"Examples"},{"location":"sql/statements/select-view/","text":"SELECT (View) \u00b6 Get a record from the specified view. The fields to get have to be already in the view. It produces one or zero static record and costs little time. Synopsis \u00b6 SELECT <* | column_name [ AS field_alias ] [, ...] > FROM view_name WHERE column_name = constant ; Notes \u00b6 Selecting from a view is a very fast operation that takes advantage of the concept of a view. So it has a more restricted syntax than selecting from a stream: The most important difference between SELECT from a stream and from a view is that the former has an EMIT CHANGES clause and the latter does not. SELECT clause can only contain * or column names with/without aliases. Other ones such as constants, arithmetical expressions, aggregate/scalar functions, etc. are not allowed. And the column names should be in the SELECT clause of the query when creating the corresponding view. FROM clause can only contain ONE view name. Clauses such as JOIN are not allowed. WHERE clause can only contain ONE conditional expression in the form of column_name = constant . And the column_name has to be the same as which in the GROUP BY clause when creating the corresponding view and the constant has to be a literal whose value is known at compile time, see constant . Examples \u00b6 // Assume that this query has been executed successfully before // CREATE VIEW my_view AS SELECT a , b , SUM ( a ), COUNT ( * ) AS cnt FROM foo GROUP BY b EMIT CHANGES ; SELECT ` SUM ( a ) ` , cnt , a FROM my_view WHERE b = 1 ;","title":"SELECT (View)"},{"location":"sql/statements/select-view/#select-view","text":"Get a record from the specified view. The fields to get have to be already in the view. It produces one or zero static record and costs little time.","title":"SELECT (View)"},{"location":"sql/statements/select-view/#synopsis","text":"SELECT <* | column_name [ AS field_alias ] [, ...] > FROM view_name WHERE column_name = constant ;","title":"Synopsis"},{"location":"sql/statements/select-view/#notes","text":"Selecting from a view is a very fast operation that takes advantage of the concept of a view. So it has a more restricted syntax than selecting from a stream: The most important difference between SELECT from a stream and from a view is that the former has an EMIT CHANGES clause and the latter does not. SELECT clause can only contain * or column names with/without aliases. Other ones such as constants, arithmetical expressions, aggregate/scalar functions, etc. are not allowed. And the column names should be in the SELECT clause of the query when creating the corresponding view. FROM clause can only contain ONE view name. Clauses such as JOIN are not allowed. WHERE clause can only contain ONE conditional expression in the form of column_name = constant . And the column_name has to be the same as which in the GROUP BY clause when creating the corresponding view and the constant has to be a literal whose value is known at compile time, see constant .","title":"Notes"},{"location":"sql/statements/select-view/#examples","text":"// Assume that this query has been executed successfully before // CREATE VIEW my_view AS SELECT a , b , SUM ( a ), COUNT ( * ) AS cnt FROM foo GROUP BY b EMIT CHANGES ; SELECT ` SUM ( a ) ` , cnt , a FROM my_view WHERE b = 1 ;","title":"Examples"},{"location":"start/cli/","text":"CLI \u00b6 Once you entered CLI, you can see the following help info: __ _________________ _________ __ ___ / / / / ___/_ __/ __ \\/ ____/ | / | / / / /_/ / \\_ _ \\ / / / /_/ / __/ / / | | / / | _/ / / __ /___/ // / / _, _/ /___/ ___ | / / / / /_/ /_//____//_/ /_/ | _/_____/_/ | _/_/ /_/ Command :h To show these help info :q To exit command line interface :help [ sql_operation ] To show full usage of sql statement SQL STATEMENTS: To create a simplest stream: CREATE STREAM stream_name ; To create a query select all fields from a stream: SELECT * FROM stream_name EMIT CHANGES ; To insert values to a stream: INSERT INTO stream_name ( field1, field2 ) VALUES ( 1 , 2 ) ; There are two kinds of commands: Basic Cli Operations, start with : SQL statements end with ; Basic CLI Operations \u00b6 To Quit current cli session: > :q To print out help info over view: > :h To show specific usage of some sql statement: > :help CREATE CREATE STREAM <stream_name> [ IF EXIST ] [ AS <select_query> ] [ WITH ( { stream_options } ) ] ; CREATE { SOURCE | SINK } CONNECTOR <stream_name> [ IF NOT EXIST ] WITH ( { connector_options } ) ; CREATE VIEW <stream_name> AS <select_query> ; Available sql operations includes: CREATE , DROP , SELECT , SHOW , INSERT , TERMINATE . SQL Statements \u00b6 All the processing and storage operations are done via SQL statements. Stream \u00b6 There are two ways to create a new data stream. \u00b6 Create an ordinary stream: CREATE STREAM stream_name ; This will create a stream with no special function. You can SELECT data from the stream and INSERT to via corresponding SQL statement. Create a stream and this stream will also run a query to select specified data from some other stream. Adding an Select statement after Create with a keyword AS can create a stream will create a stream which processing data from another stream. For example: CREATE STREAM stream_name AS SELECT * from demo EMIT CHANGES ; In the example above, by adding an AS followed by a SELECT statement to the normal CREATE operation, it will create a stream which will also select all the data from demo. After Creating the stream, we can insert values into the stream. \u00b6 INSERT INTO stream_name ( field1 , field2 ) VALUES ( 1 , 2 ); There is no restriction on the number of fields a query can insert. Also, the type of value are not restricted. However, you do need to make sure that the number of fields and the number of values are aligned. Select data from a stream \u00b6 When we have a stream, we can select data from the stream in real-time. All the data inserted after the select query is created will be print out when the insert operation happens. Select supports real-time processing on the data inserted to the stream. For example, we can choose the field and filter the data selected from the stream. SELECT a FROM demo EMIT CHANGES ; This will only select field a from stream demo. Terminate a query \u00b6 A query can be terminated if the we know the query id: TERMINATE QUERY < id > ; We can get all the query information by command SHOW : SHOW QUERIES ; output just for demonstration : \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 queryId \u2502 queryInfo \u2502 queryInfoExtra \u2502 queryStatus \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 \u2502 createdTime: \u2502 \u2502 \u2502 \u2502 \u2502 1 .626143326e9 \u2502 \u2502 status: \u2502 \u2502 810932205589156 \u2502 sqlStatement: \u2502 PlainQuery: \u2502 Running \u2502 \u2502 \u2502 SELECT * FROM \u2502 foo \u2502 timeCheckpoint: \u2502 \u2502 \u2502 foo EMIT \u2502 \u2502 1 .626143717e9 \u2502 \u2502 \u2502 CHANGES ; \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Find the query to terminate, make sure is id not already terminated, and pass the query id to TERMINATE QUERY Or under some circumstances, you can choose to TERMINATE ALL ; . Delete a stream \u00b6 Deletion command is DROP STREAM <Stream_name> ; , which deletes a stream, and terminate all the queries that depends on the stream. For example: SELECT * FROM demo EMIT CHANGES ; will be terminated if the stream demo is deleted; DROP STREAM demo ; If you try to delete a stream that does not exist, an error message will be returned. To turn it off, you can use add IF EXISTS after the stream_name: DROP STREAM demo IF EXISTS ; Show all streams \u00b6 You can also show all streams by using the SHOW STREAMS command. View \u00b6 View is a projection of specified data from streams. For example, CREATE VIEW v_demo AS SELECT SUM ( a ) FROM demo GROUP BY a EMIT CHANGES ; the above command will create a view which keep track of the sum of a (which have the same values,because of group by) and have the same value from the point this query is executed. The operations on view are very similar to those on streams. Except we can not use SELECT ... EMIT CHANGES performed on streams, because a view is static and there are no changes to emit. Instead, for example we select from view with: SELECT * FROM v_demo WHERE a = 1 ; This will print the sum of a when a = 1. If we want to create a view to record the sum of a s, we can: CREATE STREAM demo2 AS SELECT a , 1 AS b FROM demo EMIT CHANGES ; CREATE VIEW v_demo2 AS SELECT SUM ( a ) FROM demo2 GROUP BY b EMIT CHANGES ; SELECT * FROM demo2 WHERE b = 1 ;","title":"CLI usage"},{"location":"start/cli/#cli","text":"Once you entered CLI, you can see the following help info: __ _________________ _________ __ ___ / / / / ___/_ __/ __ \\/ ____/ | / | / / / /_/ / \\_ _ \\ / / / /_/ / __/ / / | | / / | _/ / / __ /___/ // / / _, _/ /___/ ___ | / / / / /_/ /_//____//_/ /_/ | _/_____/_/ | _/_/ /_/ Command :h To show these help info :q To exit command line interface :help [ sql_operation ] To show full usage of sql statement SQL STATEMENTS: To create a simplest stream: CREATE STREAM stream_name ; To create a query select all fields from a stream: SELECT * FROM stream_name EMIT CHANGES ; To insert values to a stream: INSERT INTO stream_name ( field1, field2 ) VALUES ( 1 , 2 ) ; There are two kinds of commands: Basic Cli Operations, start with : SQL statements end with ;","title":"CLI"},{"location":"start/cli/#basic-cli-operations","text":"To Quit current cli session: > :q To print out help info over view: > :h To show specific usage of some sql statement: > :help CREATE CREATE STREAM <stream_name> [ IF EXIST ] [ AS <select_query> ] [ WITH ( { stream_options } ) ] ; CREATE { SOURCE | SINK } CONNECTOR <stream_name> [ IF NOT EXIST ] WITH ( { connector_options } ) ; CREATE VIEW <stream_name> AS <select_query> ; Available sql operations includes: CREATE , DROP , SELECT , SHOW , INSERT , TERMINATE .","title":"Basic CLI Operations"},{"location":"start/cli/#sql-statements","text":"All the processing and storage operations are done via SQL statements.","title":"SQL Statements"},{"location":"start/cli/#stream","text":"","title":"Stream"},{"location":"start/cli/#there-are-two-ways-to-create-a-new-data-stream","text":"Create an ordinary stream: CREATE STREAM stream_name ; This will create a stream with no special function. You can SELECT data from the stream and INSERT to via corresponding SQL statement. Create a stream and this stream will also run a query to select specified data from some other stream. Adding an Select statement after Create with a keyword AS can create a stream will create a stream which processing data from another stream. For example: CREATE STREAM stream_name AS SELECT * from demo EMIT CHANGES ; In the example above, by adding an AS followed by a SELECT statement to the normal CREATE operation, it will create a stream which will also select all the data from demo.","title":"There are two ways to create a new data stream."},{"location":"start/cli/#after-creating-the-stream-we-can-insert-values-into-the-stream","text":"INSERT INTO stream_name ( field1 , field2 ) VALUES ( 1 , 2 ); There is no restriction on the number of fields a query can insert. Also, the type of value are not restricted. However, you do need to make sure that the number of fields and the number of values are aligned.","title":"After Creating the stream, we can insert values into the stream."},{"location":"start/cli/#select-data-from-a-stream","text":"When we have a stream, we can select data from the stream in real-time. All the data inserted after the select query is created will be print out when the insert operation happens. Select supports real-time processing on the data inserted to the stream. For example, we can choose the field and filter the data selected from the stream. SELECT a FROM demo EMIT CHANGES ; This will only select field a from stream demo.","title":"Select data from a stream"},{"location":"start/cli/#terminate-a-query","text":"A query can be terminated if the we know the query id: TERMINATE QUERY < id > ; We can get all the query information by command SHOW : SHOW QUERIES ; output just for demonstration : \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 queryId \u2502 queryInfo \u2502 queryInfoExtra \u2502 queryStatus \u2502 \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561 \u2502 \u2502 createdTime: \u2502 \u2502 \u2502 \u2502 \u2502 1 .626143326e9 \u2502 \u2502 status: \u2502 \u2502 810932205589156 \u2502 sqlStatement: \u2502 PlainQuery: \u2502 Running \u2502 \u2502 \u2502 SELECT * FROM \u2502 foo \u2502 timeCheckpoint: \u2502 \u2502 \u2502 foo EMIT \u2502 \u2502 1 .626143717e9 \u2502 \u2502 \u2502 CHANGES ; \u2502 \u2502 \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Find the query to terminate, make sure is id not already terminated, and pass the query id to TERMINATE QUERY Or under some circumstances, you can choose to TERMINATE ALL ; .","title":"Terminate a query"},{"location":"start/cli/#delete-a-stream","text":"Deletion command is DROP STREAM <Stream_name> ; , which deletes a stream, and terminate all the queries that depends on the stream. For example: SELECT * FROM demo EMIT CHANGES ; will be terminated if the stream demo is deleted; DROP STREAM demo ; If you try to delete a stream that does not exist, an error message will be returned. To turn it off, you can use add IF EXISTS after the stream_name: DROP STREAM demo IF EXISTS ;","title":"Delete a stream"},{"location":"start/cli/#show-all-streams","text":"You can also show all streams by using the SHOW STREAMS command.","title":"Show all streams"},{"location":"start/cli/#view","text":"View is a projection of specified data from streams. For example, CREATE VIEW v_demo AS SELECT SUM ( a ) FROM demo GROUP BY a EMIT CHANGES ; the above command will create a view which keep track of the sum of a (which have the same values,because of group by) and have the same value from the point this query is executed. The operations on view are very similar to those on streams. Except we can not use SELECT ... EMIT CHANGES performed on streams, because a view is static and there are no changes to emit. Instead, for example we select from view with: SELECT * FROM v_demo WHERE a = 1 ; This will print the sum of a when a = 1. If we want to create a view to record the sum of a s, we can: CREATE STREAM demo2 AS SELECT a , 1 AS b FROM demo EMIT CHANGES ; CREATE VIEW v_demo2 AS SELECT SUM ( a ) FROM demo2 GROUP BY b EMIT CHANGES ; SELECT * FROM demo2 WHERE b = 1 ;","title":"View"},{"location":"start/configuration-overview/","text":"TODO \u00b6","title":"Configurarion Overview"},{"location":"start/configuration-overview/#todo","text":"","title":"TODO"},{"location":"start/dashboard-overview/","text":"TODO \u00b6","title":"Dashboard Overview"},{"location":"start/dashboard-overview/#todo","text":"","title":"TODO"},{"location":"start/quickstart-with-docker/","text":"Quickstart with Docker \u00b6 Requirement \u00b6 Start HStream requires an operating system kernel version greater than at least Linux 4.14 Installation \u00b6 Install docker \u00b6 Note If you have already installed docker, you can skip this step. See Install Docker Engine , and install it for your operating system. Please carefully check that you meet all prerequisites. Confirm that the Docker daemon is running: docker version Tips On Linux, Docker needs root privileges. You can also run Docker as a non-root user, see Post-installation steps for Linux . Pull docker images \u00b6 docker pull hstreamdb/hstream Start a local standalone HStream-Server in Docker \u00b6 Warning Do NOT use this configuration in your production environment! Create a directory for storing db datas \u00b6 mkdir /dbdata Tips If you are a non-root user, that you can not create directory under the root, you can also create it anywhere as you can, but you need to pass the absolute data path to docker volume arguments. Start HStream Storage \u00b6 docker run -td --rm --name some-hstream-store -v /dbdata:/data/store --network host hstreamdb/hstream ld-dev-cluster --root /data/store --use-tcp Start HStreamDB Server \u00b6 docker run -it --rm --name some-hstream-server -v /dbdata:/data/store --network host hstreamdb/hstream hstream-server --port 6570 --store-config /data/store/logdevice.conf Start HStreamDB's interactive SQL CLI \u00b6 docker run -it --rm --name some-hstream-cli -v /dbdata:/data/store --network host hstreamdb/hstream hstream-client --port 6570 If everything works fine, you will enter an interactive CLI and see help information like / / / / ___/_ __/ __ \\/ ____/ | / |/ / / /_/ /\\__ \\ / / / /_/ / __/ / /| | / /|_/ / / __ /___/ // / / _, _/ /___/ ___ |/ / / / /_/ /_//____//_/ /_/ |_/_____/_/ |_/_/ /_/ Command :h To show these help info :q To exit command line interface :help [sql_operation] To show full usage of sql statement SQL STATEMENTS: To create a simplest stream: CREATE STREAM stream_name; To create a query select all fields from a stream: SELECT * FROM stream_name EMIT CHANGES; To insert values to a stream: INSERT INTO stream_name (field1, field2) VALUES (1, 2); > Create a stream \u00b6 What we are going to do first is create a stream by CREATE STREAM query. CREATE STREAM demo ; Run a continuous query over the stream \u00b6 Now we can run a continuous query over the stream we just created by SELECT query. The query will output all records from the demo stream whose humidity is above 70 percent. SELECT * FROM demo WHERE humidity > 70 EMIT CHANGES ; It seems that nothing happened. But do not worry because there is no data in the stream now. Next, we will fill the stream with some data so the query can produce output we want. Start another CLI session \u00b6 Start another CLI session, this CLI will be used for inserting data into the stream. docker exec -it some-hstream-cli hstream-client --port 6570 Insert data into the stream \u00b6 Run each of the given INSERT query in the new CLI session and keep an eye on the CLI session created in (2). INSERT INTO demo ( temperature , humidity ) VALUES ( 22 , 80 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 15 , 20 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 31 , 76 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 5 , 45 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 27 , 82 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 28 , 86 ); If everything works fine, the continuous query will output matching records in real time: {\"temperature\":22,\"humidity\":80} {\"temperature\":31,\"humidity\":76} {\"temperature\":27,\"humidity\":82} {\"temperature\":28,\"humidity\":86}","title":"Quickstart with Docker"},{"location":"start/quickstart-with-docker/#quickstart-with-docker","text":"","title":"Quickstart with Docker"},{"location":"start/quickstart-with-docker/#requirement","text":"Start HStream requires an operating system kernel version greater than at least Linux 4.14","title":"Requirement"},{"location":"start/quickstart-with-docker/#installation","text":"","title":"Installation"},{"location":"start/quickstart-with-docker/#install-docker","text":"Note If you have already installed docker, you can skip this step. See Install Docker Engine , and install it for your operating system. Please carefully check that you meet all prerequisites. Confirm that the Docker daemon is running: docker version Tips On Linux, Docker needs root privileges. You can also run Docker as a non-root user, see Post-installation steps for Linux .","title":"Install docker"},{"location":"start/quickstart-with-docker/#pull-docker-images","text":"docker pull hstreamdb/hstream","title":"Pull docker images"},{"location":"start/quickstart-with-docker/#start-a-local-standalone-hstream-server-in-docker","text":"Warning Do NOT use this configuration in your production environment!","title":"Start a local standalone HStream-Server in Docker"},{"location":"start/quickstart-with-docker/#create-a-directory-for-storing-db-datas","text":"mkdir /dbdata Tips If you are a non-root user, that you can not create directory under the root, you can also create it anywhere as you can, but you need to pass the absolute data path to docker volume arguments.","title":"Create a directory for storing db datas"},{"location":"start/quickstart-with-docker/#start-hstream-storage","text":"docker run -td --rm --name some-hstream-store -v /dbdata:/data/store --network host hstreamdb/hstream ld-dev-cluster --root /data/store --use-tcp","title":"Start HStream Storage"},{"location":"start/quickstart-with-docker/#start-hstreamdb-server","text":"docker run -it --rm --name some-hstream-server -v /dbdata:/data/store --network host hstreamdb/hstream hstream-server --port 6570 --store-config /data/store/logdevice.conf","title":"Start HStreamDB Server"},{"location":"start/quickstart-with-docker/#start-hstreamdbs-interactive-sql-cli","text":"docker run -it --rm --name some-hstream-cli -v /dbdata:/data/store --network host hstreamdb/hstream hstream-client --port 6570 If everything works fine, you will enter an interactive CLI and see help information like / / / / ___/_ __/ __ \\/ ____/ | / |/ / / /_/ /\\__ \\ / / / /_/ / __/ / /| | / /|_/ / / __ /___/ // / / _, _/ /___/ ___ |/ / / / /_/ /_//____//_/ /_/ |_/_____/_/ |_/_/ /_/ Command :h To show these help info :q To exit command line interface :help [sql_operation] To show full usage of sql statement SQL STATEMENTS: To create a simplest stream: CREATE STREAM stream_name; To create a query select all fields from a stream: SELECT * FROM stream_name EMIT CHANGES; To insert values to a stream: INSERT INTO stream_name (field1, field2) VALUES (1, 2); >","title":"Start HStreamDB's interactive SQL CLI"},{"location":"start/quickstart-with-docker/#create-a-stream","text":"What we are going to do first is create a stream by CREATE STREAM query. CREATE STREAM demo ;","title":"Create a stream"},{"location":"start/quickstart-with-docker/#run-a-continuous-query-over-the-stream","text":"Now we can run a continuous query over the stream we just created by SELECT query. The query will output all records from the demo stream whose humidity is above 70 percent. SELECT * FROM demo WHERE humidity > 70 EMIT CHANGES ; It seems that nothing happened. But do not worry because there is no data in the stream now. Next, we will fill the stream with some data so the query can produce output we want.","title":"Run a continuous query over the stream"},{"location":"start/quickstart-with-docker/#start-another-cli-session","text":"Start another CLI session, this CLI will be used for inserting data into the stream. docker exec -it some-hstream-cli hstream-client --port 6570","title":"Start another CLI session"},{"location":"start/quickstart-with-docker/#insert-data-into-the-stream","text":"Run each of the given INSERT query in the new CLI session and keep an eye on the CLI session created in (2). INSERT INTO demo ( temperature , humidity ) VALUES ( 22 , 80 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 15 , 20 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 31 , 76 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 5 , 45 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 27 , 82 ); INSERT INTO demo ( temperature , humidity ) VALUES ( 28 , 86 ); If everything works fine, the continuous query will output matching records in real time: {\"temperature\":22,\"humidity\":80} {\"temperature\":31,\"humidity\":76} {\"temperature\":27,\"humidity\":82} {\"temperature\":28,\"humidity\":86}","title":"Insert data into the stream"}]}